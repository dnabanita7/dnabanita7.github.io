<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://dnabanita7.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dnabanita7.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-04-05T02:54:52+00:00</updated><id>https://dnabanita7.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">CLIP: Contrastive Language-Image Pre-training</title><link href="https://dnabanita7.github.io/blog/2021/CLIP/" rel="alternate" type="text/html" title="CLIP: Contrastive Language-Image Pre-training" /><published>2021-02-26T00:00:00+00:00</published><updated>2021-02-26T00:00:00+00:00</updated><id>https://dnabanita7.github.io/blog/2021/CLIP</id><content type="html" xml:base="https://dnabanita7.github.io/blog/2021/CLIP/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>CLIP consists of an image encoder (ResNet or ViT) and a text encoder (Transformer).</li>
  <li>CLIP is pre-trained to predict which caption goes with which image using <strong>contrastive learning</strong>.</li>
  <li>CLIP enables <strong>zero-shot transfer</strong> to other image classification task (using the class name as input text).</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>The recent development of modern pre-training methods in NLP (e.g., T5<d-cite key="T5"></d-cite>, GPT-3<d-cite key="GPT-3"></d-cite>) suggests that the aggregate supervision within web-scale collections of text surpasses that of high-quality crowd-labeled NLP datasets.</p>

<p>Using <strong>natural language supervision</strong> for image representation:</p>

<ul>
  <li>Exciting as proofs of concept, but is still rare. This is likely because the performance on common benchmarks is much lower than SOTA.</li>
  <li>This line of work represents the middle ground between learning from a limited amount of supervised “gold labels” and learning from practically unlimited amounts of raw text.</li>
  <li>Compromise: use static softmax classifiers (fixed # of output classes) and lack a mechanism for dynamic outputs, which limits their “zero-shot” capabilities.</li>
</ul>

<p>In this work, the authors</p>

<ul>
  <li>create a new dataset of <strong>400 million (image, text) pairs</strong>;</li>
  <li>demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch;</li>
  <li>transfer the model to downstream tasks in a <strong>zero-shot</strong> manner.</li>
</ul>

<h2 id="methods">Methods</h2>

<div class="l-page" style="text-align:center;">
  <img src="https://raw.githubusercontent.com/openai/CLIP/main/CLIP.png" width="100%" style="margin-bottom: 12px; background-color: white;" />
  <p>Summary of CLIP.</p>
</div>

<h3 id="natural-language-supervision">Natural Language Supervision</h3>

<p>At the core of CLIP is the idea of learning perception from supervision contained in natural language.</p>

<p>Learning from natural language has several potential strengths over other training methods.</p>

<ul>
  <li>Easier to scale natural language supervision compared to standard crowd-sourced labeling for image classification</li>
  <li>Learning from natural language also has an important advantage over most unsupervised or self-supervised learning approaches in that it doesn’t “just” learn a representation but also connects that representation to language which enables flexible zero-shot transfer.</li>
</ul>

<h3 id="dataset">Dataset</h3>

<p>Create a new dataset: WebImageText (WIT)</p>

<ul>
  <li><strong>400M image-text pairs</strong>,</li>
  <li>Collect from publicly available sources on the Internet,</li>
  <li>Search for (image, text) pairs whose text includes one of a set of 500,000 queries (frequent words in Wikipedia),</li>
  <li>Class balance the results by including up to 20,000 (image, text),</li>
  <li>The text data has a similar word count as the WebText dataset used to train GPT-2<d-cite key="GPT-2"></d-cite>.</li>
</ul>

<h3 id="efficient-pre-training-method">Efficient Pre-Training Method</h3>

<h4 id="initial-approach">Initial Approach</h4>

<p>Jointly train an image CNN and text transformer from scratch to predict the caption of an image.</p>

<h4 id="problem">Problem</h4>

<p>This approach learns to recognize ImageNet classes three times slower than a much simpler baseline that predicts a bag-of-words encoding of the same text.</p>

<h4 id="reason">Reason</h4>

<p>The models try to predict the <em>exact words</em> of the text accompanying each image. This is a <em>difficult task</em> due to the wide variety of descriptions, comments, and related text that co-occur with images.</p>

<h4 id="solution-contrastive-representation-learning">Solution: Contrastive Representation Learning</h4>

<p>Given a batch of \(N\) (image, text) pairs, CLIP is trained to predict which of the \(N\times N\) possible (image, text) pairings across a batch actually occurred.</p>

<p>To do this, CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the \(N\) real pairs in the batch while minimizing the cosine similarity of the embeddings of the \(N^2 - N\) incorrect pairings (see the pseudocode below).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># image_encoder - ResNet or Vision Transformer
</span>  <span class="c1"># text_encoder - CBOW or Text Transformer
</span>  <span class="c1"># I[n, h, w, c] - minibatch of aligned images
</span>  <span class="c1"># T[n, l] - minibatch of aligned texts
</span>  <span class="c1"># W_i[d_i, d_e] - learned proj of image to embed
</span>  <span class="c1"># W_t[d_t, d_e] - learned proj of text to embed
</span>  <span class="c1"># t - learned temperature parameter
</span>  <span class="c1"># extract feature representations of each modality
</span>  <span class="n">I_f</span> <span class="o">=</span> <span class="nf">image_encoder</span><span class="p">(</span><span class="n">I</span><span class="p">)</span> <span class="c1">#[n, d_i]
</span>  <span class="n">T_f</span> <span class="o">=</span> <span class="nf">text_encoder</span><span class="p">(</span><span class="n">T</span><span class="p">)</span> <span class="c1">#[n, d_t]
</span>  <span class="c1"># joint multimodal embedding [n, d_e]
</span>  <span class="n">I_e</span> <span class="o">=</span> <span class="nf">l2_normalize</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">I_f</span><span class="p">,</span> <span class="n">W_i</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">T_e</span> <span class="o">=</span> <span class="nf">l2_normalize</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">T_f</span><span class="p">,</span> <span class="n">W_t</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="c1"># scaled pairwise cosine similarities [n, n]
</span>  <span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">I_e</span><span class="p">,</span> <span class="n">T_e</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
  <span class="c1"># symmetric loss function
</span>  <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
  <span class="n">loss_i</span> <span class="o">=</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">loss_t</span> <span class="o">=</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_i</span> <span class="o">+</span> <span class="n">loss_t</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
</code></pre></div></div>

<p>CLIP was trained from scratch without initializing the image encoder with ImageNet weights or the text encoder with pre-trained weights.</p>

<p>Only used linear projection to map image/text representation to the multi-model embedding space (instead of non-linear projection)</p>

<h4 id="model">Model</h4>

<ul>
  <li><strong>Image encoder:</strong> ResNet / ViT</li>
  <li><strong>Text encoder:</strong> Transformer<d-cite key="Transformer"></d-cite></li>
</ul>

<h2 id="experiments">Experiments</h2>

<h3 id="zero-shot-transfer">Zero-Shot Transfer</h3>

<h4 id="zero-shot-classification">Zero-Shot Classification</h4>

<p>Using class name as input text and predict the most probable pair (maximizing cosine similarity)</p>

<p>Results:</p>

<ul>
  <li>CLIP outperforms Visual N-Grams.</li>
  <li>The best <strong>zero-shot</strong> CLIP model achieves an accuracy of 76.2% on ImageNet, matching the performance of the original <strong>supervised</strong> ResNet-50.</li>
</ul>

<h4 id="prompt-engineering">Prompt Engineering</h4>

<p>The input texts during training are sentences while the label of an image is just a word. To help bridge this distribution gap, the authors use the prompt template <code class="language-plaintext highlighter-rouge">"A photo of a {label}"</code>. Also, zero-shot performance can be significantly improved by customizing the prompt text for each task.</p>

<h3 id="representation-learning">Representation Learning</h3>

<p>Methods to evaluate the quality of learned representation:</p>

<ol>
  <li>Fitting a linear classifier on the representation extracted from the model and measuring its performance on various datasets.</li>
  <li>Fine-tunning end-to-end</li>
</ol>

<p>The first method is used in CLIP since fine-tuning end-to-end would change the representation and potentially mask some failures.</p>

<h3 id="robustness-to-natural-distribution-shift">Robustness to Natural Distribution Shift</h3>

<p>DL models are exceedingly adept at finding correlations and patterns which hold across their training dataset and thus improve in-distribution performance. However many of these correlations and patterns are actually spurious and do not hold for other distributions and result in large drops in performance on other datasets.</p>

<ul>
  <li>
    <p><em>Effective robustness</em> measures improvements in accuracy under distribution shift above what is predicted by the documented relationship between in-distribution and out-of-distribution accuracy.</p>
  </li>
  <li>
    <p><em>Relative robustness</em> captures any improvement in out-of-distribution accuracy.</p>
  </li>
</ul>

<h4 id="zero-shot">Zero-Shot</h4>

<p>Intuitively, a zero-shot model should not be able to exploit spurious correlations or patterns that hold only on a specific distribution, since it is not trained on that distribution.</p>

<p>Results show that zero-shot models can be much more robust, however, they do not necessarily mean that supervised learning on ImageNet causes a robustness gap. Other details of CLIP, such as its large and diverse pre-training dataset or use of natural language supervision could also result in much more robust models regardless of whether they are zero-shot or fine-tuned.</p>

<h4 id="fine-tune-on-imagenet">Fine-Tune on ImageNet</h4>

<p>Although adapting CLIP to the ImageNet distribution increases its ImageNet accuracy by 9.2% to 85.4% overall, and ties the accuracy of the 2018 SOTA, <em>average accuracy under distribution shift slightly decreases</em>.</p>

<h4 id="flexible-classes">Flexible Classes</h4>

<p>The target classes across the transfer datasets are not always perfectly aligned with those of ImageNet. With CLIP we can instead generate a custom zero-shot classifier for each dataset directly based on its class names.</p>

<p>Results: This improves average effective robustness by 5% but is concentrated in large improvements on only a few datasets.</p>

<h4 id="few-shot">Few-Shot</h4>

<ul>
  <li>Few-shot CLIP also increases effective robustness compared to existing ImageNet models.</li>
  <li>But few-shot CLIP is less robust than zero-shot CLIP.</li>
  <li>Minimizing the amount of ImageNet training data used for adaption increases effective robustness at the cost of decreasing relative robustness.</li>
</ul>

<h2 id="limitations">Limitations</h2>

<ul>
  <li>The zero-shot CLIP is on average competitive with the simple <em>supervised</em> baseline of a <em>linear classifier</em> on top of ResNet-50 features, and is well below the overall SOTA. Significant work is still needed to improve the task-learning and transfer capabilities of CLIP.</li>
  <li>The performance of CLIP is poor on fine-grained classification (e.g., differentiating models of cars), abstract and systematic tasks (e.g., number counting), and novel tasks which are unlikely to be included in the pre-training dataset.</li>
  <li>The zero-shot CLIP still generalizes poorly to data that is truly out-of-distribution for it. This suggests CLIP does little to address the underlying problem of brittle generalization of deep learning models. Instead, CLIP tries to circumvent the problem and hopes that by training on such a large and varied dataset that all data will be effectively in-distribution.</li>
  <li>CLIP is still limited to choosing from only those concepts in a given zero-shot classifier. This is a significant restriction compared to a truly flexible approach like image captioning which could generate novel outputs.</li>
  <li>CLIP also does not address the poor data efficiency of deep learning.</li>
  <li>Despite our focus on zero-shot transfer, we repeatedly queried performance on full validation sets to guide the development of CLIP.</li>
  <li>Many complex tasks and visual concepts can be difficult to specify just through text.</li>
  <li>CLIP does not optimize for few-shot performance directly, resulting in a counter-intuitive drop in performance when transitioning from a zero-shot to a few-shot setting. This is different from the human performance which shows a large increase from a zero to a one-shot setting.</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Learning Transferable Visual Models From Natural Language Supervision]]></summary></entry><entry><title type="html">DDIM</title><link href="https://dnabanita7.github.io/blog/2020/DDIM/" rel="alternate" type="text/html" title="DDIM" /><published>2020-10-06T00:00:00+00:00</published><updated>2020-10-06T00:00:00+00:00</updated><id>https://dnabanita7.github.io/blog/2020/DDIM</id><content type="html" xml:base="https://dnabanita7.github.io/blog/2020/DDIM/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>This work generalizes DDPMs<d-cite key="DDPM"></d-cite> via a class of <strong>non-Markovian</strong> diffusion processes that lead to the same training objective.</li>
  <li>These non-Markovian processes interpolate between the original DDPMs and implicit models (DDIM) that have deterministic generative processes.</li>
  <li>With the same training procedure as DDPMs, this work provides a more efficient way for sampling by only considering a subsequence of latent variables.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Problems with DDPMs:</p>

<ul>
  <li>DDPMs require many iterations (~1000, the same as the number of forward steps) to produce a high-quality sample.</li>
  <li>The DDPM objective only depends on the “marginals” \(q(x_t\vert x_0)\), but not directly on the “joint” \(q(x_{1:T}\vert x_0)\). There are many inference distributions (joints) with the same marginals</li>
</ul>

<p>Ideas:</p>

<ul>
  <li>Explore non-Markovian inference processes, for which we are still able to design suitable reverse generative Markov chains.</li>
  <li>Show that the resulting variational training objectives have a shared surrogate objective, which is exactly the objective used to train DDPM.</li>
  <li>The shared objective allows us to choose from a large family of generative models using the same neural network simply by choosing a different, non-Markovian diffusion process.</li>
  <li>We are able to use non-Markovian diffusion processes which lead to “short” generative Markov chains that can be simulated in a small number of steps.</li>
</ul>

<h2 id="methods">Methods</h2>

<h3 id="non-markovian-forward-processes">Non-Markovian Forward Processes</h3>

<p>Define a family \(\mathcal{Q}\) of (inference) distributions, indexed by vector \(\sigma\in\mathbb{R}^T_{\ge0}\):</p>

\[q_\sigma(x_{1:T}\vert x_0):=q_\sigma(x_T\vert x_0)\prod_{t=2}^T q_\sigma(x_{t-1}\vert x_t,x_0)\]

<p>where</p>

\[\begin{aligned}
q_\sigma(x_T\vert x_0)&amp;=\mathcal{N}(\sqrt{\alpha_T}x_0,(1-\alpha_T)I),\\
q_\sigma(x_{t-1}\vert x_t, x_0)&amp;=\mathcal{N}(\sqrt{\alpha_{t-1}}x_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\frac{x_t-\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}},\sigma_t^2I)\quad\text{for all } t&gt;1.
\end{aligned}\]

<p>Remarks:</p>

<ol>
  <li>The mean is chosen so that \(q_\sigma(x_t\vert x_0)=\mathcal{N}(\sqrt{\alpha_t}x_0,(1-\alpha_t)I)\) (see <a href="#the-inference-distribution">Appendix</a>).</li>
  <li>The joint is factorized in reverse order.</li>
  <li>The forward process \(q_\sigma(x_t\vert x_{t-1}, x_0)\) can be derived from Bayes’rule, which is also Gaussian, but is non-Markovian in comparison to DDPM<d-cite key="DDPM"></d-cite>.</li>
  <li>The variance \(\sigma_t^2\) of \(q_\sigma(x_{t-1}\vert x_t, x_0)\) is a hyperparameter that can be choosen. In contrast, the variance of \(q(x_{t-1}\vert x_t, x_0)\) in DDPM is determined by \(\alpha\) due to the Markovian model.</li>
</ol>

<h3 id="generative-process">Generative Process</h3>

<p>Define a trainable generative process</p>

\[p_\theta(x_{0:T}):=p_\theta(x_T)\prod_{i=1}^Tp_\theta^{(t)}(x_{t-1}\vert x_t),\]

<p>where each \(p_\theta^{(t)}(x_{t-1}\vert x_t)\) leverages knowledge of \(q_\sigma(x_{t-1}\vert x_t, x_0)\).</p>

<ol>
  <li>Given a noisy observation \(X_t\), e.g., \(X_t=\sqrt{\alpha_t}X_0+\sqrt{1-\alpha_t}\epsilon_t\) with \(X_0\sim q(x_0)\) and \(\epsilon_t\sim\mathcal{N}(0,I)\).</li>
  <li>
    <p>Make a prediction of the corresponding \(X_0\): The model \(\epsilon_\theta^{(t)}(x_t)\) predicts \(\epsilon_t\) from \(X_t\), without knowing \(X_0\). Then we can predict the <em>denoised observation</em>, which is a prediction of \(X_0\) given \(X_t\),</p>

\[f_\theta^{(t)}(x_t):=\frac{1}{\sqrt{\alpha_t}}(x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)).\]
  </li>
  <li>
    <p>Use the prediction to obtain to sample \(X_{t-1}\) from the reverse conditional distribution \(q_\sigma(x_{t-1}\vert x_t,x_0)\): we can define the generative process with a fixed prior \(p_\theta(x_T)=\mathcal{N}(0,I)\) and</p>

\[p_\theta^{(t)}(x_{t-1}\vert x_t)=\left\{
 \begin{aligned}
 &amp;\mathcal{N}(f_\theta^{(1)}(x_1),\sigma_1^2 I)\quad&amp;\text{if }t=1,\\
 &amp;q_\sigma(x_{t-1}\vert x_t,f_\theta^{(t)}(x_t))\quad&amp;\text{if }t\ge1,
 \end{aligned}
 \right.\]

    <p>where Gaussian noise is added to the case of \(t=1\) to ensure that the generative process is supported everywhere.</p>
  </li>
</ol>

<p>Remarks: This generative process is basically the same as DDPM with some minor differences</p>

<ul>
  <li>This work uses \(q_\sigma(x_{t-1}\vert x_t,x_0)\) while DDPM uses \(q(x_{t-1}\vert x_t, x_0)\).</li>
  <li>This work uses the same variance \(\sigma_t^2\) for \(q_\sigma(x_{t-1}\vert x_t,x_0)\) and \(p_\theta^{(t)}(x_{t-1}\vert x_t)\) while DDPM introduce \(\sigma_t^2\) in \(p_\theta^{(t)}(x_{t-1}\vert x_t)\), which might be different from the \(\tilde\beta_t\) in \(q(x_{t-1}\vert x_t, x_0)\).</li>
</ul>

<h3 id="unified-variational-inference-objective">Unified Variational Inference Objective</h3>

<p>The parameters \(\theta\) are optimized via the variational inference objective</p>

\[J_\sigma(\epsilon_\theta):=\mathbb{E}_{q_\sigma}[\log q_\sigma(X_{1:T}\vert X_0)-\log p_\theta(X_{0:T})]\]

<p>In comparison, DDPM optimizes the following objective:</p>

\[L_\gamma(\epsilon_\theta):=\sum_{t=1}^T\gamma_t \mathbb{E}_{X_0,\epsilon_t}\left[\|\epsilon_\theta^{(t)}(\sqrt{\alpha_t}X_0+\sqrt{1-\alpha_t}\epsilon_t)-\epsilon_t\|_2^2\right],\]

<p>where \(\gamma\in\mathbb{R}^T_{&gt;0}\) is a vector of positive coefficients in the objective that depends on \(\alpha_{1:T}\). In DDPM The objective with \(\gamma=1\) is optimized instead to maximize the generation performance of the trained model.</p>

<p><strong>Theorem 1.</strong> For all \(\sigma\in\mathbb{R}^T_{&gt;0}\), there exists \(\gamma\in\mathbb{R}^T_{&gt;0}\) and \(C\in\mathbb{R}\), such that \(J_\sigma=L_\gamma+C\). (see <a href="#proof-of-theorem-1">Appendix</a> for the proof)</p>

<p>Discussion:</p>

<ul>
  <li>If parameters \(\theta\) are not shared across different \(t\), the optimal solution to \(L_\gamma\) will not depend on the weights \(\gamma\) as  global
optimum is achieved by separately maximizing each term in the sum</li>
  <li>This property justified the use of \(L_1\) (i.e., \(\gamma=1\)) as a surrogate objective function for the variational lower bound in DDPMs.</li>
  <li>Since \(J_\sigma\) is equivalent to some \(L_\gamma\) from Theorem 1, the optimal solution of \(J\sigma\) is also the same as that of \(L_1\).</li>
  <li>Therefore, if parameters are not shared across \(t\), then the \(L_1\) objective used by DDPMs can be used as a surrogate objective for the variational objective \(J_\sigma\) as well.</li>
</ul>

<h3 id="sampling-from-generalized-generative-processes">Sampling from Generalized Generative Processes</h3>

<p>With \(L_1\) as the objective (\(\sigma\) does not appear in the loss), we are not only learning a generative process for the Markovian inference process considered DDPM, but also generative processes for many non-Markovian forward processes parametrized by \(\sigma\) that described above.</p>

<p>Use pre-trained DDPM models as the solutions to the new objectives, and focus on finding a generative process that is better at producing samples subject to our needs by changing \(\sigma\).</p>

<h4 id="denoising-diffusion-implicit-models">Denoising Diffusion Implicit Models</h4>

<p>Generate a sample \(x_{t-1}\) from a sample \(x_t\):</p>

\[x_{t-1}=
\underbrace{\sqrt{\alpha_{t-1}}\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right)}_{\text{predicted }x_0}
+
\underbrace{\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\epsilon_\theta^{(t)}(x_t)}_{\text{direction pointing to }x_t}
+
\underbrace{\sigma_t\epsilon_t}_\text{random noise}\]

<p>where \(\epsilon_t\sim\mathcal{N}(0, I)\). Different choices of \(\sigma\) result in different generative processes, all while using the same model \(\epsilon_\theta\), so re-training the model is unnecessary.</p>

<ul>
  <li>DDPM
    <ul>
      <li>Set \(\sigma_t=\sqrt{(1-\alpha_{t-1})/(1-\alpha_t)}\sqrt{1-\alpha_t/\alpha_{t-1}}\).</li>
      <li>The forward process becomes Markovian.</li>
    </ul>
  </li>
  <li>DDIM (denoising diffusion implict model)
    <ul>
      <li>Set \(\sigma_t=0\) for all \(t\).</li>
      <li>The forward process becomes deterministic given \(x_{t-1}\) and \(x_{0}\).</li>
      <li>Samples are generated from latent variables with a fixed procedure (from \(x_T\) to \(x_0\)).</li>
    </ul>
  </li>
</ul>

<h4 id="accelerated-generation-processes">Accelerated Generation Processes</h4>

<p>The generative process is considered as the approximation to the reverse
process, and therefore, they should have the same number of time steps \(T\).</p>

<p>However, as \(L_1\) does not depend on the specific forward procedure as long as \(q_\sigma(x_t\vert x_0)\) is fixed, we may also consider forward processes with lengths smaller than \(T\), which accelerates the corresponding generative processes without having to train a different model.</p>

<ol>
  <li>Consider a subset \(\{X_{\tau_1},\dots X_{\tau_S}\}\), where \(\tau\) is an increasing sub-sequence of \([1,\dots, T]\) of length \(S\).</li>
  <li>Define the a forward process over \(X_\tau\) such that \(q(x_{\tau_i}\vert x_0)=\mathcal{N}(\sqrt{\alpha_{\tau_i}}x_0, (1-\alpha_{\tau_i})I)\) matches the “marginals”.</li>
  <li>The generative process now sampled latent variable according to reversed \(\tau\) (<em>sampling trajectory</em>).</li>
</ol>

<p>Details can be found in the <a href="#accelerated-sampling-processes">Appendix</a></p>

<p>Insight:</p>

<ul>
  <li>In principle, we can train a model with an arbitrary number of forward steps but only sample from some of them in the generative process.</li>
  <li>Therefore, the trained model could consider many more steps or even a continuous time variable \(t\)</li>
</ul>

<h4 id="relevance-to-neural-odes">Relevance to Neural ODEs</h4>

<p>The DDIM iterate (i.e., \(\sigma_t=0\)):</p>

\[x_{t-1}=
\sqrt{\alpha_{t-1}}\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right)
+
\sqrt{1-\alpha_{t-1}}\cdot\epsilon_\theta^{(t)}(x_t)\]

<p>can be rewritten as</p>

\[\frac{x_{t-\Delta t}}{\sqrt{\alpha_{t-\Delta t}}}=\frac{x_t}{\sqrt{\alpha_t}}+\left(\sqrt{\frac{1-\alpha_{t-\Delta t}}{\alpha_{t-\Delta t}}}-\sqrt{\frac{1-\alpha_t}{\alpha_t}}\right)\epsilon_\theta^{(t)}(x_t)\]

<p>We can reparameterize \(\sqrt{(1-\alpha)/\alpha}\) with \(\omega\) and \(x/\sqrt{\alpha}\) with \(\bar{x}\). When \(\Delta t\rightarrow 0\), \(\omega\) and \(\bar{x}\) are functions of \(t\), where \(\omega\) is continous, increasing with \(\omega(0)=0\). The above iteration can be treated as an Euler method over the following ODE:</p>

\[\text{d}\bar{x}(t)=\epsilon^{(t)}_\theta\left(\frac{\bar{x}(t)}{\sqrt{\omega^2+1}}\right)\text{d}\omega(t),\]

<p>where the initial conditions is \(\bar{x}(T)=x(T)/\sqrt{\alpha(T)}\sim\mathcal{N}(0,1/\alpha(T))\). Since \(\alpha(T)\approx 0\), The variance \(1/\alpha(T)\) would be very large.</p>

<h2 id="experiments">Experiments</h2>

<p>Key results:</p>

<ul>
  <li>DDIMs outperform DDPMs in terms of image generation <em>when fewer iterations are considered</em>, giving speed-ups of 10x to 100x over the original DDPM generation process.</li>
  <li>Unlike DDPMs, once the initial latent variables \(x_T\) are fixed, DDIMs retain high-level image features regardless of the generation trajectory (different sub-sequences), so they are able to perform interpolation directly from the latent space.</li>
  <li>DDIMs can also be used to encode samples that reconstruct them from the latent code, which DDPMs cannot do due to the stochastic sampling process.</li>
</ul>

<p>Setup:</p>

<ul>
  <li>Use the <strong>same trained model</strong>
    <ul>
      <li>number of time steps \(T=1000\)</li>
      <li>trained with \(L_1\)</li>
    </ul>
  </li>
  <li><strong>The only change</strong> is how to produce samples from the model by controlling
    <ul>
      <li>how fast the samples are obtained, \(τ\)</li>
      <li>and sample variance \(\sigma_t^2\), which interpolates between the deterministic DDIM and the stochastic DDPM.
\(\sigma_{\tau_i}(\eta)=\eta\sqrt{\frac{1-\alpha_{\tau_{i-1}}}{1-\alpha_{\tau_i}}}\sqrt{1-\frac{\alpha_{\tau_i}}{\alpha_{\tau_{i-1}}}}\)
where \(\eta&gt;0\) is a hyperparameter. This includes DDPM (\(\eta=1\)), DDIM (\(\eta=0\)), and DDPM with larger variance (denoted as \(\hat{\sigma}:\hat{\sigma}_{\tau_i}=\sqrt{1-\alpha_{\tau_i}/\alpha_{\tau_{i-1}}}\)).</li>
    </ul>
  </li>
</ul>

<h3 id="sample-quality-and-efficiency">Sample Quality and Efficiency</h3>

<p>Vary the number of timesteps used to generate a sample (\(S=\text{dim}(\tau)\)) and the stochasticity of the process \(\eta\), and present a tradeoff between sample quality and computational costs.</p>

<p>Results:</p>

<ul>
  <li>DDIM (\(\eta=0\)) achieves the best sample quality when \(S\) is small.</li>
  <li>DDPM (\(\eta=1\) and \(\hat{\sigma}\)) typically has worse sample quality compared to its less stochastic counterparts with the same \(S\). (when \(S&lt;T\))</li>
  <li>In the case with \(S=T=1000\), DDPM (\(\hat{\sigma}\)) is better than DDIM.</li>
  <li>The sample quality of DDPM (\(\hat{\sigma}\)) becomes much worse for smaller \(S\), which suggests that it is ill-suited for shorter trajectories.</li>
  <li>DDIM achieves high sample quality much more consistently.</li>
  <li>DDIM is able to produce samples with quality comparable to 1000 step models within 20 to 100 steps.</li>
</ul>

<h3 id="sample-consistency-in-ddims">Sample Consistency in DDIMs</h3>

<p>For DDIM, the generative process is deterministic, and \(x_0\) would depend only on the initial state \(x_T\).</p>

<p>Compare generated images under different generative trajectories (i.e. different \(\tau\)) while starting with the same initial \(x_T\)</p>

<p>Results:</p>

<ul>
  <li>For the generated images with the same initial \(x_T\), most high-level features are similar, regardless of the generative trajectory.</li>
  <li>It indicates that \(x_T\) alone would be an informative latent encoding of the image.</li>
  <li>Minor details that affect sample quality are encoded in the parameters.</li>
</ul>

<h3 id="interpolation-in-deterministic-generative-processes">Interpolation in Deterministic Generative Processes</h3>

<p>Since the high-level features of the DDIM sample are encoded by \(x_T\), it might be used for semantic interpolation.</p>

<p>This is different from the interpolation procedure in DDPM, where the same \(x_T\) would lead to highly diverse \(x_0\) due to the stochastic generative process</p>

<p>DDIM is able to control the generated images on a high level directly through the latent variables, which DDPMs cannot.</p>

<h3 id="reconstruction-from-latent-space">Reconstruction from Latent Space</h3>

<p>As DDIM is the Euler integration for a particular ODE, it should be able to encode from \(x_0\) to \(x_T\) (reverse of the ODE) and reconstruct \(x_0\) from the resulting \(x_T\) (forward of the ODE).</p>

<p>Results: DDIMs have lower reconstruction error for larger \(S\) and have properties similar to Neural ODEs and normalizing flows. The same cannot be said for DDPMs due to their stochastic nature.</p>

<h2 id="appendix">Appendix</h2>

<h3 id="comparision-between-notations-in-ddpm-and-ddim">Comparision between Notations in DDPM and DDIM</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">meaning</th>
      <th style="text-align: center">DDPM<d-cite key="DDPM"></d-cite></th>
      <th style="text-align: center">DDIM (this work)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">diffusion rate</td>
      <td style="text-align: center">\(\beta_t\)</td>
      <td style="text-align: center">\(1-\alpha_t/\alpha_{t-1}\)</td>
    </tr>
    <tr>
      <td style="text-align: center">1-diffusion rate</td>
      <td style="text-align: center">\(\alpha_t\)</td>
      <td style="text-align: center">\(\alpha_t/\alpha_{t-1}\)</td>
    </tr>
    <tr>
      <td style="text-align: center">product of 1-diffusion rate</td>
      <td style="text-align: center">\(\overline{\alpha}_t\)</td>
      <td style="text-align: center">\(\alpha_t\)</td>
    </tr>
  </tbody>
</table>

<h3 id="marginal-and-conditional-gaussians">Marginal and Conditional Gaussians</h3>

<p>The materials in this section are from Pattern Recognition and Machine Learning (Bishop, 2006)  Section 2.3.3.</p>

<p>Given a marginal Gaussian distribution for \(x\) and a conditional Gaussian distribution for \(y\) given \(x\) in the form</p>

\[\begin{aligned}
p(x)&amp;=\mathcal{N}(x;\mu,\Lambda^{-1})\\
p(y\vert x)&amp;=\mathcal{N}(y; Ax+b,L^{-1}) \\
\end{aligned}\]

<p>The marginal distribution of \(y\) and the conditional distribution of \(x\) given \(y\) are given by</p>

\[\begin{aligned}
p(y) &amp;= \mathcal{N}(y; A\mu+b, L^{-1}+A\Lambda^{-1}A^T)\\
p(x|y) &amp;= \mathcal{N}(x; \Sigma\{A^TL(y-b)+\Lambda\mu\}, \Sigma)
\end{aligned}\]

<p>where \(\Sigma=(\Lambda + A^TLA)^{-1}\).</p>

<h3 id="the-inference-distribution">The Inference Distribution</h3>

<p>The core of the inference distribution \(q_\sigma\) is the conditional distribution of \(X_{t-1}\) given \(X_t\) and \(X_0\), i.e.,</p>

\[q_\sigma(x_{t-1}\vert x_t, x_0)=\mathcal{N}(\tilde{\mu}_t(x_t,x_0),\sigma_t^2I),\]

<p>where \(\tilde\mu_t\) is the mean function. Assuming it takes a linear form, i.e., \(\tilde\mu_t(x_t, x_0)=ax_t+bx_0\), where \(a\) and \(b\) are constants to be determined. We want the proposed joint distribution to match the “marginals” of the original DM. Specifically, suppose \(q_\sigma(x_t\vert x_0)=\mathcal{N}(\sqrt{\alpha_t} x_0, (1-\alpha_t)I)\), we want \(q_\sigma(x_{t-1}\vert x_0)=\mathcal{N}(\sqrt{\alpha_{t-1}} x_0, (1-\alpha_{t-1})I)\). We can compute \(q_\sigma(x_{t-1}\vert x_0)\) from \(q_\sigma(x_{t}\vert x_0)\) and \(q_\sigma(x_{t-1}\vert x_t,x_0)\) as follows. (see <a href="#marginal-and-conditional-gaussians">this section</a>)</p>

\[q_\sigma(x_{t-1}\vert x_0)=\mathcal{N}(a\sqrt{\alpha_t}x_0+bx_0,[\sigma_t^2+(1-\alpha_{t})a^2]I)\]

<p>We solve the following equations to match the mean and the variance:
\(\begin{aligned}
a\sqrt{\alpha_t}+b&amp;=\sqrt{\alpha_{t-1}}\\
\sigma_t^2+(1-\alpha_{t})a^2&amp;=1-\alpha_{t-1}
\end{aligned}\)</p>

<p>which givens</p>

\[\begin{aligned}
a&amp;=\frac{\sqrt{1-\alpha_{t-1}-\sigma_t^2}}{\sqrt{1-\alpha_{t}}}\\
b&amp;=\sqrt{\alpha_{t-1}}-\frac{\sqrt{\alpha_t}\sqrt{1-\alpha_{t-1}-\sigma_t^2}}{\sqrt{1-\alpha_{t}}}
\end{aligned}\]

<p>Therefore,</p>

\[\tilde\mu_t=a x_t+bx_0=\sqrt{\alpha_{t-1}}x_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\frac{x_t-\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_{t}}}\]

<p>and</p>

\[q_\sigma(x_{t-1}\vert x_t, x_0)=\mathcal{N}(\sqrt{\alpha_{t-1}}x_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\frac{x_t-\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}},\sigma_t^2I)\]

<p>In comparison, DDPM uses different mean and variance for \(q(x_{t-1}\vert x_t, x_0)\):</p>

\[\begin{aligned}
q(x_{t-1}\vert x_t, x_0)&amp;=\mathcal{N}\left(\frac{\sqrt{\alpha_{t-1}}}{1-\alpha_{t}}\left(1-\frac{\alpha_t}{\alpha_{t-1}}\right)x_0 + \frac{\sqrt{\alpha_t}(1-\alpha_{t-1})}{\sqrt{\alpha_{t-1}}(1-\alpha_t)}x_t, \frac{1-\alpha_{t-1}}{1-\alpha_t}(1-\frac{\alpha_t}{\alpha_{t-1}})I\right) \\
&amp;=\mathcal{N}\left(\sqrt{\alpha_{t-1}}x_0+\frac{\sqrt{\alpha_t}(1-\alpha_{t-1})}{\sqrt{\alpha_{t-1}}\sqrt{1-\alpha_t}}\cdot\frac{x_t-\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}},\frac{1-\alpha_{t-1}}{1-\alpha_t}(1-\frac{\alpha_t}{\alpha_{t-1}})I \right)
\end{aligned}\]

<p>If we set</p>

\[\sigma_t^2=\frac{1-\alpha_{t-1}}{1-\alpha_t}\left(1-\frac{\alpha_t}{\alpha_{t-1}}\right),\]

<p>then \(q_\sigma(x_{t-1}\vert x_t, x_0)=q(x_{t-1}\vert x_t, x_0)\) and the model becomes DDPM.</p>

<h3 id="proof-of-theorem-1">Proof of Theorem 1</h3>

<p>For all \(\sigma\in\mathbb{R}^T_{&gt;0}\), there exists \(\gamma\in\mathbb{R}^T_{&gt;0}\) and \(C\in\mathbb{R}\), such that \(J_\sigma=L_\gamma+C\).</p>

<p>Proof:</p>

<p>Following the derivation of DDPM (where \(\equiv\) denotes “equal up to a value that does not depend on \(\theta\), but may depend on \(q_\sigma\)”).</p>

\[\begin{aligned}
J_\sigma(\epsilon_\theta)&amp;:=\mathbb{E}_{q_\sigma}[\log q_\sigma(X_{1:T}\vert X_0)-\log p_\theta(X_{0:T})] \\
&amp;\equiv\mathbb{E}_{q_\sigma}\left[\sum_{t=2}^T D_\text{KL}(q_\sigma(x_{t-1}|X_t,X_0)||p_\theta^{(t)}(x_{t-1}|X_t)) -\log p_\theta^{1}(X_0|X_1)\right]\\
\end{aligned}\]

<p>For \(t&gt;1\):</p>

\[\begin{aligned}
\mathbb{E}_{q_\sigma}\left[ D_\text{KL}(q_\sigma(x_{t-1}|X_t,X_0)||p_\theta^{(t)}(x_{t-1}|X_t))\right]&amp;=\mathbb{E}_{X_0,X_t}\left[ D_\text{KL}(q_\sigma(x_{t-1}|X_t,X_0)||q_\sigma(x_{t-1}|X_t,f_\theta^{t}(X_t)))\right]\\
&amp;\equiv\mathbb{E}_{X_0,X_t}\left[\frac{\|\tilde{\mu}_t(X_t,X_0)-\tilde{\mu}_t(X_t,f_\theta^{(t)}(X_t))\|_2^2}{2\sigma_t^2}\right]\\
&amp;=\mathbb{E}_{X_0,X_t}\left[\frac{b_t^2}{2\sigma_t^2}\|X_0-f_\theta^{(t)}(X_t)\|_2^2\right]\\
&amp;=\mathbb{E}_{X_0,\epsilon}\left[\frac{b_t^2(1-\alpha_t)}{2\sigma_t^2\alpha_t}\|\epsilon-\epsilon_\theta^{(t)}(X_t)\|_2^2\right]\\
\end{aligned}\]

<p>For \(t=1\):</p>

\[\begin{aligned}
\mathbb{E}_{q_\sigma}\left[ -\log p_\theta^{1}(X_0|X_1)\right]&amp;\equiv\mathbb{E}_{X_0,X_t}\left[\frac{1}{2\sigma_t^2}\|X_0-f_\theta^{(t)}(X_1)\|_2^2\right]\\
&amp;=\mathbb{E}_{X_0,\epsilon}\left[\frac{(1-\alpha_t)}{2\sigma_t^2\alpha_t}\|\epsilon-\epsilon_\theta^{(t)}(X_1)\|_2^2\right]\\
\end{aligned}\]

<p>Choosing \(\gamma_1=(1-\alpha_t)/(2\sigma_t^2\alpha_t)\) and \(\gamma_t=(1-\alpha_t)b_t^2/(2\sigma_t^2\alpha_t)\) for \(t&gt;1\), we have \(J_\sigma(\epsilon_\theta)\equiv L_\gamma(\epsilon_\theta)\).</p>

<h3 id="accelerated-sampling-processes">Accelerated Sampling Processes</h3>

<p>The inference process in the accelerated case is given by</p>

\[q_{\sigma,\tau}(x_{1:T}\vert x_0)=q_{\sigma, \tau}(x_{\tau_S}\vert x_0)\prod_{i=2}^S q_{\sigma, \tau}(x_{\tau_{i-1}}\vert x_{\tau_i}, x_0)\prod_{t\in\overline\tau}q_{\sigma, \tau}(x_t|x_0),\]

<p>where \(\tau\) is a sub-sequence of \([1,\dots, T]\) of length \(S\) with \(\tau_S=T\), and \(\overline\tau:=\{1,\dots, T\}\backslash \tau\), i.e., the graphical model of \(\{X_{\tau_i}\}_{i=1}^S\) and \(X_0\) form a chain, whereas the graphical model of \(\{X_t\}_{t\in\overline\tau}\) and \(X_0\) form a star graph.</p>

<p>Define:</p>

\[\begin{aligned}
q_{\sigma,\tau}(x_t\vert x_0)&amp;=\mathcal{N}(\sqrt{\alpha_t}x_0,(1-\alpha_t)I)\quad\forall t\in\overline\tau\cup\{T\}\\
q_{\sigma,\tau}(x_{\tau_{i-1}}\vert x_{\tau_i},x_0)&amp;=\mathcal{N}(\sqrt{\alpha_{\tau_{i-1}}}x_0+\sqrt{1-\alpha_{\tau_{i-1}}-\sigma_{\tau_i}^2}\cdot\frac{x_{\tau_i}-\sqrt{\alpha_{\tau_i}}x_0}{\sqrt{1-\alpha_{\tau_i}}},\sigma_{\tau_i}^2I),\quad 2\le i\le S
\end{aligned}\]

<p>where the coefficients are chosen such that:</p>

\[q_{\sigma,\tau}(x_{\tau_i}|x_0)=\mathcal{N}(\sqrt{\alpha_{\tau_i}}x_0,(1-\alpha_{\tau_i})I)\quad 1\le i\le S,\]

<p>i.e., the “marginals” match.</p>

<p>The corresponding “generative process” is defined as:</p>

\[p_\theta(x_{0:T}):=
\underbrace{p_\theta(x_T)\prod_{i=1}^Sp_{\theta}^{(\tau_i)}(x_{\tau_{i-1}}\vert x_{\tau_i})}_\text{use to produce samples}
\times
\underbrace{\prod_{t\in\overline\tau}p_\theta^{(t)}(x_0\vert x_t)}_\text{use in objective},\]

<p>where only part of the models are actually being used to produce samples (define \(\tau_0=0\)). The conditionals are:</p>

\[\begin{aligned}
p_\theta^{\tau_{i}}(x_{\tau_{i-1}}\vert x_{\tau_i})&amp;=q_{\sigma,\tau}(x_{\tau_{i-1}}\vert x_{\tau_i}, f_\theta^{(\tau_i)}(x_{\tau_i}))\quad\text{if }i\in\{2,\dots, S\}\\
p_\theta^{(t)}(x_0\vert x_t)&amp;=\mathcal{N}(f_\theta^{(t)}(x_{t}),\sigma_{t}^2I),\quad\text{if }t\in\overline\tau\cup\{\tau_1\},
\end{aligned}\]

<p>which leverages \(q_{\sigma,\tau}(x_{\tau_{i-1}}\vert x_{\tau_i}, x_0)\) as part of the inference process.</p>

<p>The resulting variational objective becomes (define \(x_{\tau_{L+}}\))</p>

\[\begin{aligned}
J_{\sigma,\tau}(\epsilon_\theta)&amp;=\mathbb{E}_{q_{\sigma,\tau}}[\log q_{\sigma,\tau}(X_{1:T}\vert X_0)-\log p_\theta(X_{0:T})]\\
&amp;\equiv\mathbb{E}_{q_{\sigma,\tau}}\left[\sum_{i=2}^S D_\text{KL}(q_{\sigma,\tau}(x_{\tau_{i-1}}|X_{\tau_i},X_0)||p_\theta^{(\tau_i)}(x_{\tau_{i-1}}|X_{\tau_i})) -\log p_\theta^{(\tau_1)}(X_0|X_{\tau_i})\right.\\
&amp;\qquad+\left. \sum_{t\in\overline\tau} -\log p_\theta^{(t)}(X_0|X_{t}) \right]\\

\end{aligned}\]

<p>A similar argument to the proof used in <a href="#proof-of-theorem-1">Theorem 1</a> can show that \(J_{\sigma,\tau}\) can also be converted to an objective of the form \(L_\gamma\).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Denoising Diffusion Implicit Models]]></summary></entry><entry><title type="html">GPT-3</title><link href="https://dnabanita7.github.io/blog/2020/GPT-3/" rel="alternate" type="text/html" title="GPT-3" /><published>2020-07-22T00:00:00+00:00</published><updated>2020-07-22T00:00:00+00:00</updated><id>https://dnabanita7.github.io/blog/2020/GPT-3</id><content type="html" xml:base="https://dnabanita7.github.io/blog/2020/GPT-3/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>Although recent large pre-training models are <strong>task-agnostic in architecture</strong>, they still require <strong>task-specific fine-tuning</strong> datasets of thousands of examples.</li>
  <li>The author train GPT-3, an autoregressive LM with 175B parameters, and test its performance in the <strong>few-shot</strong> setting, i.e., providing task descriptions and few-shot demonstrations purely via text interaction (prompt), and without any gradient updates.</li>
  <li>This work shows that scaling up LM greatly improves <strong>task-agnostic</strong>, <strong>few-shot</strong> performance, sometimes even reaching competitiveness with prior SOTA finetuning approaches.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>The trend of pre-trained language representation NLP</p>
<ol>
  <li>single-layer pre-trained word embedding + task-specific architectures</li>
  <li>multiple layers of representations (e.g. RNN) + task-specific architectures</li>
  <li>pre-train RNNs or Transformers, and then directly fine-tune, them without task-specific architectures.</li>
</ol>

<h3 id="problems-with-pre-training--fine-tune">Problems with Pre-training + Fine-tune</h3>

<ul>
  <li>Although the last paradigm uses <strong>task-agnostic</strong> architectures, they still require <strong>task-specific</strong> fine-tuning.</li>
  <li>The need for a large dataset of labeled examples for every new task limits the applicability of LMs.</li>
  <li>The potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution.</li>
  <li>Humans do not require large supervised datasets to learn most language tasks. A brief directive in natural language + a tiny number of examples is often sufficient.</li>
</ul>

<h3 id="meta-learning">Meta Learning</h3>

<p>In the context of LMs, Meta learning means the model develops a broad set of skills at training time and then uses those abilities at inference time to rapidly adapt to or recognize the desired task.</p>

<p>GPT-2<d-cite key="GPT-2"></d-cite> attempts to do this via what “<em>in-context learning</em>”: the model is conditioned on natural language instruction and/or a few demonstrations of the task.</p>

<p>While it has shown some initial promise, this approach still achieves results far inferior to fine-tuning</p>

<p>This work shows that scaling up language models greatly improves <em>task-agnostic</em>, <em>few-shot</em> performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches.</p>

<h3 id="model-scale">Model Scale</h3>

<table>
  <tbody>
    <tr>
      <td>Model</td>
      <td>GPT<d-cite key="GPT"></d-cite></td>
      <td>BERT<d-cite key="BERT"></d-cite></td>
      <td>GPT-2<d-cite key="GPT-2"></d-cite></td>
      <td>Megatron-LM<d-cite key="Megatron-LM"></d-cite></td>
      <td>T5<d-cite key="T5"></d-cite></td>
      <td>Turing-NLG<d-cite key="Turing-NLG"></d-cite></td>
    </tr>
    <tr>
      <td># of parameters</td>
      <td>100M</td>
      <td>300M</td>
      <td>1.5B</td>
      <td>8B</td>
      <td>11B</td>
      <td>17B</td>
    </tr>
  </tbody>
</table>

<p>There is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale. Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.</p>

<p>The authors test this hypothesis by training a <strong>175B</strong> parameter autoregressive language model (GPT-3) and measuring its in-context learning abilities (few-shot, one-shot, and zero-shot).</p>

<h2 id="methods">Methods</h2>

<h3 id="different-settings">Different Settings</h3>

<h4 id="fine-tuning-ft">Fine-Tuning (FT)</h4>

<ul>
  <li>A pre-trained model by training on a supervised dataset specific to the desired task. Typically thousands to hundreds of thousands of labeled examples are used.</li>
  <li><em>The main advantage</em> is strong performance on many benchmarks.</li>
  <li><em>The main disadvantages</em> are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution, and the potential to exploit spurious features of the training data, potentially resulting in an unfair comparison with human performance.</li>
  <li>In this work, the authors do not fine-tune GPT-3.</li>
</ul>

<h4 id="few-shot-fs">Few-Shot (FS)</h4>

<ul>
  <li>Refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning, but no weight updates are allowed.</li>
  <li>The number of samples \(K\) is in the range of 10 to 100 as this is how many examples can fit in the model’s context window (<code class="language-plaintext highlighter-rouge">nctx = 2048</code>).</li>
  <li>The main advantages: A major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset.</li>
  <li>The main disadvantage: Results from this method have so far been much worse than SOTA fine-tuned models. Also, a small amount of task-specific data is still required.</li>
</ul>

<h4 id="one-shot-1s">One-Shot (1S)</h4>

<ul>
  <li>It is the same as few-shot except that only one demonstration is allowed, in addition to a natural
language description of the task.</li>
  <li>The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.</li>
</ul>

<h4 id="zero-shot-0s">Zero-Shot (0S)</h4>

<ul>
  <li>No demonstrations are allowed, and the model is only given a natural language instruction describing the task.</li>
  <li>This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations.</li>
  <li>But it is also the most challenging setting.</li>
</ul>

<h3 id="model">Model</h3>

<p>The same model and architecture as GPT-2, with the exception that</p>

<ol>
  <li>GPT-3 uses alternating dense and locally banded sparse attention patterns in the layers of the transformer.</li>
  <li>To study the dependence of ML performance on model size, 8 different sizes of model were trained, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model called GPT-3.</li>
</ol>

<h3 id="training-dataset">Training Dataset</h3>

<p>Unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, the authors took 3 steps to improve the average quality of the datasets:</p>

<ol>
  <li>Downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora;</li>
  <li>Performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of held-out validation set as an accurate measure of overfitting;</li>
  <li>Added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.</li>
</ol>

<p>The overall training dataset has about 500B tokens.</p>

<h2 id="experiments">Experiments</h2>

<h3 id="evaluation">Evaluation</h3>

<p>For few-shot learning, the authors evaluate each example in the evaluation set by randomly drawing \(K\) examples from that task’s training set as conditioning</p>

<h4 id="multiple-choice-problems">Multiple-Choice Problems</h4>

<p>Provide \(K\) examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion.</p>

<p>For most tasks, the per-token likelihood (to normalize for length) is compared. However, sometime it might be beneficial to normalize by the unconditional probability of each completion, by computing</p>

\[\frac{P(\texttt{completion}|\texttt{context})}{P(\texttt{completion}|\texttt{answer context})},\]

<p>where answer context is the string <code class="language-plaintext highlighter-rouge">"Answer: "</code> or <code class="language-plaintext highlighter-rouge">"A: "</code>.</p>

<h4 id="binary-classification">Binary Classification</h4>

<p>Give the options more semantically meaningful names (e.g. <code class="language-plaintext highlighter-rouge">"True"</code> or <code class="language-plaintext highlighter-rouge">"False"</code> rather than 0 or 1) and then treat the task like multiple choice.</p>

<h4 id="free-form-completion">Free-Form Completion</h4>

<p>Use beam search with a beam width of 4 and a length penalty of \(\alpha = 0.6\).</p>

<h3 id="language-modeling-cloze-and-completion">Language Modeling, Cloze, and Completion</h3>

<h4 id="language-modeling">Language Modeling</h4>

<p>Evaluate the zero-shot GPT-3 by computing the perplexity on the Penn Tree Bank dataset. GPT-3 sets a new SOTA compared to GPT-2.</p>

<h4 id="lambada">LAMBADA</h4>

<p>Task: The model is asked to predict the last word of sentences which requires reading a paragraph of context.</p>

<p>The authors use a fill-in-the-blank format to guide GPT-3 to predict a word rather than other valid continuations of the paragraph:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Alice was friends with Bob. Alice went to visit her friend ___. → Bob
George bought some baseball equipment, a ball, a glove, and a ___. →
</code></pre></div></div>

<p>Results:</p>

<ul>
  <li>GPT-3 achieves new SOTA on LAMBADA.</li>
  <li>Few-shot performance improves strongly with model size.</li>
  <li>The one-shot setting always performs worse than the zero-shot setting.</li>
</ul>

<h4 id="hellaswag">HellaSwag</h4>

<p>Task: Pick the best ending to a story or set of instructions.</p>

<p>Results: The performance of GPT-3 on this task is a fair amount lower than the overall SOTA.</p>

<h4 id="storycloze">StoryCloze</h4>

<p>Task: Select the correct ending sentence for a five-sentence long story.</p>

<p>Results: GPT-3 is better than previous zero-shot results but still underperforms fine-tuned SOTA.</p>

<h3 id="question-answering">Question Answering</h3>

<p><em>open-book QA:</em> use an information retrieval system to find relevant text and train a model to generate an answer given the question and the retrieved text.</p>

<p><em>closed-book QA:</em> train a model to answer the questions directly.</p>

<p>Results:</p>

<ul>
  <li>Overall, on one of the three datasets GPT-3’s one-shot matches the open-book fine-tuning SOTA.</li>
  <li>On the other two datasets, it approaches the performance of the closed-book SOTA despite not using fine-tuning.</li>
</ul>

<h3 id="translation">Translation</h3>

<p>For GPT-2 a filter was used on a multilingual collection of documents to produce an English-only dataset due to capacity concerns. Since the capacity increases by over two orders of magnitude from GPT-2 to GPT-3, the scope of the training dataset is also expanded to include more representation of other languages. the majority of the data is derived from raw Common Crawl with only quality-based filtering. Although GPT-3’s training data is still primarily English (93% by word count), it also includes 7% of text in other languages.</p>

<p>Zero-shot/one-shot/few-shot GPT-3 underperforms, nears competitive performance, and achieves similar average performance to prior unsupervised NMT work.</p>

<p>GPT-3 has a noticeable skew in its performance depending on language direction. GPT-3 significantly outperforms prior unsupervised NMT work when translating into English but underperforms when translating in the other direction.</p>

<h3 id="winograd-style-tasks">Winograd-Style Tasks</h3>

<p>Task: Determine which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human.</p>

<h3 id="common-sense-reasoning">Common Sense Reasoning</h3>

<p>Task: Capture physical or scientific reasoning</p>

<p>Results: Overall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks.</p>

<h3 id="reading-comprehension">Reading Comprehension</h3>

<p>Results:</p>

<ul>
  <li>A wide spread is observed in GPT-3’s performance across 5 datasets suggestive of varying capability with different answer formats.</li>
  <li>In general, GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.</li>
</ul>

<h3 id="superglue">SuperGLUE</h3>

<p>Results: The average performance of few-shot GPT-3 matches that of a fine-tuned BERT model.</p>

<h3 id="natural-language-inference">Natural Language Inference</h3>

<p>NLI concerns the ability to understand the relationship between two sentences.</p>

<p>Task: a two or three class classification problem where the model classifies whether the second sentence logically follows from the first, contradicts the first sentence, or is possibly true (neutral)</p>

<p>Results: NLI is still a very difficult task for language models and they are only just beginning to show signs of progress.</p>

<h3 id="synthetic-and-qualitative-tasks">Synthetic and Qualitative Tasks</h3>

<h4 id="arithmetic">Arithmetic</h4>

<p>Results: Overall, GPT-3 displays reasonable proficiency at moderately complex arithmetic in few-shot, one-shot, and even zero-shot settings.</p>

<h4 id="word-scrambling-and-manipulation-tasks">Word Scrambling and Manipulation Tasks</h4>

<p>Each task involves giving the model a word distorted by some combination of scrambling, addition, or deletion of characters, and asking it to recover the original word.</p>

<p>Results:</p>

<ul>
  <li>The one-shot performance is significantly weaker than the few-shot setting.</li>
  <li>In the zero-shot setting, the model can rarely perform any of the tasks. This suggests that the model really does appear to learn these tasks at test time, as the model cannot perform them zero-shot and their artificial nature makes them unlikely to appear in the pre-training data.</li>
</ul>

<h4 id="news-article-generation">News Article Generation</h4>

<p>Few-shot learning: Provide three previous news articles and the title and subtitle of a proposed next article in the model’s context to condition it.</p>

<p>Results:</p>

<ul>
  <li>With prompt, the model is able to reliably generate short articles in the “news” genre.</li>
  <li>Human abilities to detect model-generated text appear to decrease as model size increases.</li>
</ul>

<h4 id="learning-and-using-novel-words">Learning and Using Novel Words</h4>

<p>Task: using a word in a sentence after seeing it defined only once.</p>

<p>Results: Overall, GPT-3 appears to be at least proficient at the task of using novel words in a sentence.</p>

<h4 id="correcting-english-grammar">Correcting English Grammar</h4>

<p>Prompt: <code class="language-plaintext highlighter-rouge">Poor English Input: &lt;sentence&gt;\n Good English Output: &lt;sentence&gt;</code>.</p>

<h2 id="limitations">Limitations</h2>

<ul>
  <li>Despite the strong quantitative and qualitative improvements of GPT-3, it still has notable weaknesses in text synthesis and several NLP tasks</li>
  <li>GPT-3 has several structural and algorithmic limitations: do not include any bidirectional architectures or other training objectives such as denoising.</li>
  <li>Scaling up any LM-like model may eventually run into (or could already be running into) the limits of the pretraining objective.</li>
  <li>Poor sample efficiency during pre-training.</li>
  <li>Ambiguity about whether few-shot learning actually learns new tasks “from scratch” at inference time, or if it simply recognizes and identifies tasks that it has learned during training.</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Language Models are Few-Shot Learners]]></summary></entry><entry><title type="html">DDPM</title><link href="https://dnabanita7.github.io/blog/2020/DDPM/" rel="alternate" type="text/html" title="DDPM" /><published>2020-06-19T00:00:00+00:00</published><updated>2020-06-19T00:00:00+00:00</updated><id>https://dnabanita7.github.io/blog/2020/DDPM</id><content type="html" xml:base="https://dnabanita7.github.io/blog/2020/DDPM/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>A diffusion probabilistic model is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time.</li>
  <li>Transitions of this chain are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the data in the opposite direction of sampling until signal is destroyed.</li>
  <li>When the diffusion rates \(\beta_t\) are small, it is sufficient to set the sampling chain transitions to conditional Gaussians too, allowing for a particularly simple neural network parameterization.</li>
  <li>DDPM shows that diffusion models are capable of generating high quality samples.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Let \(X_0\sim q(x_0)\) be the data distribution. The forward process (diffusion process) \(q(x_{1:T}\vert x_0)\) of diffusion models is a Markov chain that generates latent variables \(X_1,\dots,X_T\) by gradually adding Gaussian noise to the data \(X_0\) according to a variance schedule \(\beta_1,\dots,\beta_T\) where</p>

\[q(x_{1:T}|x_0):=\prod_{t=1}^T q(x_t|x_{t-1}),\quad q(x_t|x_{t-1}):=\mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_t I)\]

<p>choices of \(\beta_t\)</p>

<ul>
  <li>learned by reparameterization</li>
  <li>held constant as hyperparameters</li>
</ul>

<p>The diffusion models aim to learn a generative distribution \(p_\theta\) to describe the same trajectory, but in <strong>reverse</strong>,</p>

\[p_\theta(x_{0:T}):=p(x_T)\prod_{t=1}^T p_\theta(x_{t-1}|x_t),\quad p(x_T)=\mathcal{N}(x_t;0,I)\]

<p>The reverse processes have the same functional form (Gaussian) when \(\beta_t\) are small (the time reversibility of SDE). Therefore, diffusion models learn Gaussian distribution in the reverse Markov chain:</p>

\[p_\theta(x_{t-1}|x_t):=\mathcal{N}(x_{t-1};\mu_\theta(x_t, t), \Sigma_\theta(x_t, t))\]

<p>Training is performed by optimizing the usual variational bound on negative log likelihood (see <a href="#variational-bound-on-negative-log-likelihood">Appendix</a> for details):</p>

\[\mathbb{E}_q[-\log p_\theta(X_0)] \le \mathbb{E}_q\left[-\log p(X_T)-\sum_{t=1}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_t|X_{t-1})} \right] =:L\\\]

<p>The loss \(L\) can be rewritten as</p>

\[L =\mathbb{E}_q\left[\underbrace{D_\text{KL}(q(x_T|X_0)||p(x_T))}_{L_T}+\sum_{t=2}^T \underbrace{D_\text{KL}(q(x_{t-1}|X_t,X_0)||p_\theta(x_{t-1}|X_t))}_{L_{t-1}} \underbrace{-\log p_\theta(X_0|X_1)}_{L_0} \right]\]

<p>When conditioned on \(X_0\), both \(X_t\) and \(X_{t-1}\) are Gaussian (see <a href="#close-form-of-forward-process">Appendix</a>):</p>

\[q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\overline{\alpha}_t}x_0,(1-\overline{\alpha}_t)I),\quad q(x_{t-1}|x_0)=\mathcal{N}(x_{t-1};\sqrt{\overline{\alpha}_{t-1}}x_0,(1-\overline{\alpha}_{t-1})I),\]

<p>where \(\alpha_t:=1-\beta_t\) and \(\overline{\alpha}_t:=\prod_{s=1}^t\alpha_s\). According to the forward model, we have the following reparameterization: \(X_t=\sqrt{\alpha_t}X_{t-1}+\sqrt{1-\alpha_t}\epsilon_t\), with \(\epsilon_t\in\mathcal{N}(0,I)\) and therefore, \(\text{Cov}(X_t,X_{t-1})=\sqrt{\alpha_t}\text{Var}(X_{t-1})\) and the posterior conditioned on \(X_0\) is</p>

\[q(x_{t-1}|x_t,x_0)=\mathcal{N}(x_{t-1};\tilde{\mu}_t(x_t,x_0),\tilde{\beta}_tI),\]

<p>where</p>

\[\tilde{\mu}_t(x_t,x_0):=\frac{\sqrt{\overline\alpha_{t-1}}\beta_t}{1-\overline\alpha_t}x_0+\frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t}x_t,\quad \tilde{\beta}_t:=\frac{1-\overline\alpha_{t-1}}{1-\overline\alpha_{t}}\beta_t\]

<h2 id="ddpm">DDPM</h2>

<h3 id="forward-process-and-l_t">Forward process and \(L_T\)</h3>

<p>Fix \(\beta_t\) to constants, the approximate posterior \(q\) has no learnable parameters, and \(L_T\) is a constant.</p>

<h3 id="reverse-process-and-l_1t-1">Reverse process and \(L_{1:T-1}\)</h3>

\[p_\theta(x_{t-1}|x_t):=\mathcal{N}(x_{t-1};\mu_\theta(x_t, t), \Sigma_\theta(x_t, t))\]

<h4 id="variances">Variances</h4>

<p>Set \(\Sigma_\theta(x_t,t)=\sigma_t^2I\) to untrained time-dependent constants. Experimentally, the following two choices of \(\sigma_t^2\) have similar results.</p>

<ul>
  <li>
    <p>Choice 1: \(\sigma_t^2=\beta_t\). This optimial for \(X_0\sim\mathcal{N}(0,I)\). (see <a href="#special-cases-for-optimal-posterior-variance">Appendix</a>)</p>
  </li>
  <li>
    <p>Choice 2: \(\sigma_t^2=\frac{1-\overline\alpha_{t-1}}{1-\overline\alpha_t}\beta_t\). This is optimal for \(X_0\) deterministically set to one point. (see <a href="#special-cases-for-optimal-posterior-variance">Appendix</a>)</p>
  </li>
</ul>

<h4 id="means">Means</h4>

<p>We rewrite \(L_{t-1}\) as follows. (see <a href="#special-cases-for-optimal-posterior-variance">Appendix</a>)</p>

\[L_{t-1}=\mathbb{E}_q\left[ \frac{1}{2\sigma_t^2}\| \tilde\mu_t(X_t, X_0) -\mu_\theta(X_t,t)\|_2^2 \right]+C\]

<p>where \(C\) is a constant that does not depend on \(\theta\). Since \(X_t=\sqrt{\overline\alpha_t}X_0+\sqrt{1-\overline\alpha_t}\epsilon\) for \(\epsilon\sim\mathcal{N}(0,I)\).</p>

\[\begin{aligned}
L_{t-1}-C&amp;=\mathbb{E}_{X_0,\epsilon}\left[ \frac{1}{2\sigma_t^2}\left\| \tilde\mu_t\left(X_t, \frac{1}{\sqrt{\overline\alpha_t}}(X_t-\sqrt{1-\overline\alpha_t}\epsilon)\right) -\mu_\theta(X_t,t)\right\|_2^2 \right]\\
&amp;=\mathbb{E}_{X_0,\epsilon}\left[ \frac{1}{2\sigma_t^2}\left\| \frac{1}{\sqrt{\alpha_t}}  \left(X_t-\frac{\beta_t}{\sqrt{1-\overline\alpha_t}}\epsilon\right)-\mu_\theta(X_t,t)\right\|_2^2 \right]
\end{aligned}\]

<p>Since \(X_t\) is available as input to the model, we may choose the parameterization</p>

\[\mu_\theta(X_t, t):= \frac{1}{\sqrt{\alpha_t}}  \left(X_t-\frac{\beta_t}{\sqrt{1-\overline\alpha_t}}\epsilon_\theta(X_t, t)\right)\]

<p>To sample \(X_{t-1}\sim p_\theta(x_{t-1}\vert x_t)\) is to compute</p>

\[X_{t-1}=\frac{1}{\sqrt{\alpha_t}}  \left(X_t-\frac{\beta_t}{\sqrt{1-\overline\alpha_t}}\epsilon_\theta(X_t, t)\right)+\sigma_t z,\]

<p>where \(z\sim\mathcal{N}(0,1)\). With the parameterization of \(\mu_\theta\), the loss simplifies to</p>

\[\mathbb{E}_{X_0,\epsilon}\left[ \frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\overline\alpha_t)}||\epsilon-\epsilon_\theta(\sqrt{\overline\alpha_t}X_0+\sqrt{1-\overline\alpha_t}\epsilon,t) ||_2^2 \right]\]

<h3 id="data-scaling-reverse-process-decoder-and-l_0">Data Scaling, Reverse Process Decoder, and \(L_0\)</h3>

<p>Assume that image data consists of integers in \(\{0,1,\dots,255\}\) and scale them ilnearly to \([-1,1]\). This ensures that the neural network reverse process operates on consistently scaled inputs starting from the standard normal prior \(p(x_T)\). Set the last term of the reverse process as follows:</p>

\[p_\theta(x_0|x_1)=\prod_{i=1}^D\int_{\delta_-(x_0^i)}^{\delta_+(x_0^i)}\mathcal{N}(x;\mu_\theta^i(x_1,1),\sigma_1^2)dx,\]

\[\delta_+(x)=\left\{
  \begin{aligned}
  &amp;\infty &amp;\text{if } x=1 \\
  &amp;x+\frac{1}{255} &amp;\text{if } x&lt;1
  \end{aligned}
  \right.,
\qquad
\delta_-(x)=\left\{
  \begin{aligned}
  &amp;-\infty &amp;\text{if } x=-1 \\
  &amp;x-\frac{1}{255} &amp;\text{if } x&gt;-1
  \end{aligned}
  \right.,\]

<p>where \(D\) is the data dimensionality and the \(i\) superscript indicates extraction of one coordinate. Similar to the discretized continuous distributions used in VAE decoders and autoregressive models, the choice here ensures that the variational bound is a lossless codelength of discrete data, without the need of adding noise to the data or incorporating the Jacobian of the scaling operation into the log likelihood.</p>

<p>At the end of sampling, \(\mu_\theta(X_1, 1)\) is outputed without adding noise.</p>

<h3 id="training-and-sampling">Training and Sampling</h3>

<p>It is beneficial to sample quality (and simpler to implement) to train on the
following variant of the variational bound:</p>

\[L_\text{simple}(\theta):=\mathbb{E}_{t,X_0,\epsilon}\left[ \| \epsilon-\epsilon_\theta(\sqrt{\overline\alpha_t}X_0+\sqrt{1-\overline\alpha_t}\epsilon,t) \|_2^2 \right],\]

<p>where \(t\) is uniform between \(1\) and \(T\).</p>

<ul>
  <li>The \(t=1\) case corresponds to \(L_0\)</li>
  <li>The \(t&gt;1\) cases correspond to an unweighted version of \(L_{t-1}\).</li>
  <li>The \(L_T\) term does not appear because the forward process variances \(\beta_t\) are fixed.</li>
</ul>

<p>Since the simplified objective discards the weighting, it is a <strong>weighted variational bound</strong> that emphasizes different aspects of reconstruction compared to the standard variational bound. The diffusion process setup in <a href="#experiments">the experiments</a> causes the simplified objective to down-weight loss terms corresponding to small \(t\). These terms train the network to denoise data with very small amounts of noise, so it is beneficial to down-weight them so that the network can focus on more difficult denoising tasks at larger \(t\) terms.</p>

<p>The overall training and sampling algorithms are as follows.</p>

<div class="l-body" style="text-align:center;">
  <img src="https://hojonathanho.github.io/diffusion/assets/img/algorithms.png" width="100%" style="margin-bottom: 12px; background-color: white;" />
  <p></p>
</div>

<h2 id="experiments">Experiments</h2>

<h3 id="hyperparameters">Hyperparameters</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">\(T\)</th>
      <th style="text-align: center">\(\beta_1\)</th>
      <th style="text-align: center">\(\beta_T\)</th>
      <th style="text-align: center">\(\beta\) intepolation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1000</td>
      <td style="text-align: center">\(10^{-4}\)</td>
      <td style="text-align: center">0.02</td>
      <td style="text-align: center">Linear</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Use a U-Net backbone with group normalization.</li>
  <li>Parameters are shared across time, which is specified to the network using the Transformer sinusoidal position embedding</li>
  <li>Use self-attention at the \(16 \times 16\) feature map resolution.</li>
</ul>

<h3 id="sample-quality">Sample Quality</h3>

<ul>
  <li>The unconditional DDPM achieves better sample quality than most models in the literature, including class conditional models.</li>
  <li>Training DDPMs on the true variational bound yields better codelengths (i.e., the negative log likelihoods) than training on the simplified objective, as expected, but the latter yields the best sample quality.</li>
</ul>

<h3 id="reverse-process-parameterization-and-training-objective-ablation">Reverse Process Parameterization and Training Objective Ablation</h3>

<ol>
  <li>The baseline option of predicting \(\tilde\mu\) works well only when trained on the true variational bound instead of unweighted mean squared error.</li>
  <li>Learning reverse process variances (by incorporating a parameterized diagonal \(\Sigma_\theta\) into the variational bound) leads to unstable training and poorer sample quality compared to fixed variances.</li>
  <li>Predicting \(\epsilon\) performs approximately as well as predicting \(\tilde\mu\) when trained on the variational bound with fixed variances, but much better when trained with the simplified objective.</li>
</ol>

<h3 id="progressive-coding">Progressive Coding</h3>

<p>The lossless codelengths of DDPMs are better than the large estimates reported for energy-based models and score matching using annealed importance sampling, they are not competitive with other types of likelihood-based generative models.</p>

<p>DDPMs have an <em>inductive bias</em> that makes them excellent <em>lossy</em> compressors.</p>

<h4 id="progressive-lossy-compression">Progressive Lossy Compression</h4>

<p>The distortion decreases steeply in the low-rate region of the rate-distortion plot, indicating that the majority of the bits are indeed allocated to imperceptible distortions</p>

<h4 id="progressive-generation">Progressive Generation</h4>

<p>Experiment: Run a progressive unconditional generation process given by progressive decompression from random bits, i.e., predicting the result of the reverse process, \(\hat{X}_0=(X_t-\sqrt{1-\overline\alpha_t}\epsilon_\theta(X_t, t))/\sqrt{\overline\alpha_t}\), while sampling from the reverse process.</p>

<p>Result: Large-scale image features appear first and details appear last</p>

<p>Experiment: Stochastic prediction by sampling images from \(p_\theta(x_0\vert x_t)\) for different time step</p>

<p>Results:  When \(t\) is small, all but fine details are preserved, and when \(t\) is large, only large-scale features are preserved. Perhaps these are hints of conceptual compression.</p>

<h4 id="connection-to-autoregressive-decoding">Connection to Autoregressive Decoding</h4>

<p>The variational bound \(L\) can be rewritten as (see <a href="#variational-bound-on-negative-log-likelihood">Appendix</a>):</p>

\[L=D_\text{KL}(q(x_T)||p(x_T)) + \mathbb{E}_q\left[\sum_{t=1}^TD_\text{KL}(q(x_{t-1}|X_t)||p_\theta(x_{t-1}|X_t))\right] + H(X_0)\]

<p>An autoregressive model can be considered as a special diffusion model:</p>

<ol>
  <li>Set the diffusion process length \(T\) to the dimensionality of the data.</li>
  <li>Define the forward process so that \(q(x_t\vert x_0)\) places all probability mass on \(x_0\) with the first \(t\) coordinates masked out (i.e., \(q(x_t\vert x_{t-1})\) masks out the \(t\)-th coordinate).</li>
  <li>Set \(p(x_t)=q(x_t)\) to place all mass on a blank image, and therefore \(D_\text{KL}(q(x_T)\|p(x_T))=0\)</li>
  <li>Take \(p_\theta(x_{t-1}\vert x_t)\) to be a fully expressive conditional distribution.</li>
  <li>With these choices, minimizing \(D_\text{KL}(q(x_{t-1}\vert x_t)\|p_\theta(x_{t-1}\vert x_t))\) trains \(p_\theta\) to copy coordinates \(t+1,\dots,T\) unchanged and to predict the \(t\)-th coordinate given \(t+1,\dots,T\)</li>
</ol>

<p>Insights:</p>

<ul>
  <li>
    <p>Interpret the Gaussian DM as a kind of autoregressive model with a <strong>generalized bit ordering</strong> that cannot be expressed by reordering data coordinates.</p>
  </li>
  <li>
    <p>Prior work has shown that such reorderings introduce inductive biases that have an impact on sample quality.</p>
  </li>
  <li>
    <p>Gaussian diffusion might serve a similar purpose, perhaps to greater effect since Gaussian noise might be more natural to add to images compared to masking noise. Gaussian diffusion length is not restricted to equal the data dimension: Gaussian diffusions can be made shorter for fast sampling or longer for model expressiveness.</p>
  </li>
</ul>

<h3 id="interpolation">Interpolation</h3>

<p>Interpolate images in the latent space</p>

<ol>
  <li>Given two images from the data distribution \(X_0,X_0'\sim q(x_0)\).</li>
  <li>Use \(q\) as a stochastic encoder to encoder \(X_0\) and \(X_0'\) into \(X_t\) and \(X_t'\) respectively, where \(X_t,X_t'\sim q(x_t\vert x_0)\).</li>
  <li>Linearly interpolate in the latent space: \(\bar{X}_t=(1-\lambda)X_t+\lambda X_t'\).</li>
  <li>Decode \(\bar{X}_t\) into the image space by the reverse process, \(\bar{X}_0\sim p(x_0\vert\bar{x}_t)\)</li>
</ol>

<p>Results:  The reverse process produces high-quality reconstructions and plausible interpolations that smoothly vary attributes such as pose, skin tone, hairstyle, expression, and background, but not eyewear. Larger \(t\) results in coarser and more varied interpolations.</p>
<h2 id="appendix">Appendix</h2>

<h3 id="variational-bound-on-negative-log-likelihood">Variational Bound on Negative log Likelihood</h3>

\[\begin{aligned}
\mathbb{E}_q[-\log p_\theta(X_0)] &amp;=\int -q(x_0)\log p_\theta(x_0) d x_0 \\
&amp; = \int -q(x_0)\log\left[\int\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}q(x_{1:T}|x_0) d x_{1:T} \right]d x_0 \\
&amp;\le \int -q(x_0)\int\log\left[\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}\right]q(x_{1:T}|x_0) d x_{1:T}  d x_0 \\
&amp; = \int -q(x_{0:T})\log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)} dx_{0:T} \\
&amp; = \mathbb{E}_q \left[-\log\frac{p_\theta(X_{0:T})}{q(X_{1:T}|X_0)}\right]  \\
&amp; = \mathbb{E}_q \left[-\log\frac{p(X_T)\prod_{t=1}^T p_\theta(X_{t-1}|X_t)}{\prod_{t=1}^Tq(X_t|X_{t-1})}\right] \\
&amp; = \mathbb{E}_q\left[-\log p(X_T)-\sum_{t=1}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_t|X_{t-1})} \right] =:L\\
\end{aligned}\]

<p>To derive the objective for DDPM, \(L\) can be rewritten as</p>

\[\begin{aligned}
L &amp; = \mathbb{E}_q\left[-\log p(X_T)-\sum_{t=1}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_t|X_{t-1})} \right]\\
&amp; =  \mathbb{E}_q\left[-\log p(X_T)-\sum_{t=2}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_t|X_{t-1})} - \log\frac{p_\theta(X_0|X_1)}{q(X_1|X_0)} \right] \\
&amp; =  \mathbb{E}_q\left[-\log p(X_T)-\sum_{t=2}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_t|X_{t-1}, X_0)} - \log\frac{p_\theta(X_0|X_1)}{q(X_1|X_0)} \right] \\
&amp; =  \mathbb{E}_q\left[-\log p(X_T)-\sum_{t=2}^T\log\frac{p_\theta(X_{t-1}|X_t)q(X_{t-1}|X_0)}{q(X_{t-1}|X_{t}, X_0)q(X_t|X_0)} - \log\frac{p_\theta(X_0|X_1)}{q(X_1|X_0)} \right] \\
&amp; =  \mathbb{E}_q\left[-\log \frac{p(X_T)}{q(X_T|X_0)}-\sum_{t=2}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_{t-1}|X_{t}, X_0)} - \log p_\theta(X_0|X_1)\right] \\
&amp; = \mathbb{E}_q\left[\underbrace{D_\text{KL}(q(x_T|X_0)||p(x_T))}_{L_T}+\sum_{t=2}^T \underbrace{D_\text{KL}(q(x_{t-1}|X_t,X_0)||p_\theta(x_{t-1}|X_t))}_{L_{t-1}} \underbrace{-\log p_\theta(X_0|X_1)}_{L_0} \right]
\end{aligned}\]

<p>Here is another way to rewrite \(L\) which is helpful for analysis.</p>

\[\begin{aligned}
L &amp; = \mathbb{E}_q\left[-\log p(X_T)-\sum_{t=1}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_t|X_{t-1})} \right]\\
&amp;=\mathbb{E}_q\left[-\log p(X_T)-\sum_{t=1}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_{t-1}|X_{t})}\frac{q(X_{t-1})}{q(X_t)} \right]\\
&amp;=\mathbb{E}_q\left[-\log \frac{p(X_T)}{q(X_T)}-\sum_{t=1}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_{t-1}|X_{t})}-\log q(X_0) \right]\\
&amp;=D_\text{KL}(q(x_T)||p(x_T)) + \mathbb{E}_q\left[\sum_{t=1}^TD_\text{KL}(q(x_{t-1}|X_t)||p_\theta(x_{t-1}|X_t))\right] + H(X_0)
\end{aligned}\]

<h3 id="close-form-of-forward-process">Close Form of Forward Process</h3>

<p>A notable property of the forward process is that it admits sampling \(X_t\) at an arbitrary timestep \(t\) in closed form: using the notation \(\alpha_t:=1-\beta_t\) and \(\overline{\alpha}_t:=\prod_{s=1}^t\alpha_s\), we have</p>

\[q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\overline{\alpha}_t}x_0,(1-\overline{\alpha}_t)I)\]

<p>Proof:</p>

<p>Let \(\epsilon_1,\dots,\epsilon_n\) be noise sampled iid from \(\mathcal{N}(0, I)\).</p>

\[\begin{aligned}
X_t&amp;=\sqrt{\alpha_t}X_{t-1}+\sqrt{1-\alpha_t}\epsilon_t \\
&amp;= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}X_{t-2}+\sqrt{1-\alpha_{t-1}}\epsilon_{t-1})+\sqrt{1-\alpha_t}\epsilon_t \\
&amp;=\dots \\
&amp;=\sqrt{\alpha_t\cdots\alpha_1}X_0+\sqrt{\alpha_t\cdots\alpha_2}\sqrt{1-\alpha_1}\epsilon_1+\cdots+\sqrt{\alpha_t}\sqrt{(1-\alpha_{t-1})}\epsilon_{t-1}+\sqrt{1-\alpha_t}\epsilon_t \\
&amp;=\sqrt{\overline{\alpha}_t}X_0+\sqrt{1-\overline\alpha_t}\overline\epsilon_t,
\end{aligned}\]

<p>where \(\overline\epsilon_t\sim\mathcal{N}(0, I)\). To obtain the last equality, we use the fact that</p>

\[\begin{aligned}
(\sqrt{\alpha_t\cdots\alpha_1})^2+(\sqrt{\alpha_t\cdots\alpha_2}\sqrt{1-\alpha_1})^2+\cdots+(\sqrt{\alpha_t}\sqrt{(1-\alpha_{t-1})})^2+(\sqrt{1-\alpha_t})^2&amp;=1\\
(\sqrt{\alpha_t\cdots\alpha_2}\sqrt{1-\alpha_1})^2+\cdots+(\sqrt{\alpha_t}\sqrt{(1-\alpha_{t-1})})^2+(\sqrt{1-\alpha_t})^2&amp;=1-\overline\alpha_t
\end{aligned}\]

<h3 id="kl-divergence-between-two-gaussian-distributions">KL Divergence between Two Gaussian Distributions</h3>

<p>Let \(p:=\mathcal{N}(\mu_p,\Sigma_p)\) and \(q:=\mathcal{N}(\mu_q,\Sigma_q)\) be two \(d\)-dimensional Gaussian distributions.</p>

\[\begin{aligned}
D_\text{KL}(p||q)&amp;=\mathbb{E}_p\left[\log(p)-\log(q)\right] \\
&amp;=\mathbb{E}_p\left[\frac{1}{2}\log\frac{|\Sigma_q|}{|\Sigma_p|}-\frac{1}{2}(X-\mu_p)^T\Sigma_p^{-1}(X-\mu_p)+\frac{1}{2}(X-\mu_q)^T\Sigma_q^{-1}(X-\mu_q)\right] \\
&amp;=\frac{1}{2}\log\frac{|\Sigma_q|}{|\Sigma_p|}-\frac{1}{2}\mathbb{E}_p\left[(X-\mu_p)^T\Sigma_p^{-1}(X-\mu_p)\right]+\frac{1}{2}\mathbb{E}_p\left[(X-\mu_q)^T\Sigma_q^{-1}(X-\mu_q)\right]
\end{aligned}\]

<p>The second term is</p>

\[\begin{aligned}
\mathbb{E}_p\left[(X-\mu_p)^T\Sigma_p^{-1}(X-\mu_p)\right]&amp;=\mathbb{E}_p\left[tr\{(X-\mu_p)^T\Sigma_p^{-1}(X-\mu_p)\}\right] \\
&amp;=\mathbb{E}_p\left[tr\{(X-\mu_p)(X-\mu_p)^T\Sigma_p^{-1}\}\right] \\
&amp;=tr\{\mathbb{E}_p\left[(X-\mu_p)(X-\mu_p)^T\Sigma_p^{-1}\right]\} \\
&amp;=tr\{\mathbb{E}_p\left[(X-\mu_p)(X-\mu_p)^T\right]\Sigma_p^{-1}\} \\
&amp; =tr\{\Sigma_p \Sigma_p^{-1}\}\\
&amp;= tr\{I\}\\
&amp;= d
\end{aligned}\]

<p>The third term is</p>

\[\begin{aligned}
\mathbb{E}_p\left[(X-\mu_q)^T\Sigma_q^{-1}(X-\mu_q)\right]&amp; =\mathbb{E}_p\left[(X-\mu_p+\mu_p-\mu_q)^T\Sigma_q^{-1}(X-\mu_p+\mu_p-\mu_q)\right]\\
&amp;=\mathbb{E}_p\left[(X-\mu_p)^T\Sigma_q^{-1}(X-\mu_p)\right] + (\mu_p-\mu_q)^T\Sigma_q^{-1}(\mu_p-\mu_q)\\
&amp;=tr\{\Sigma_p \Sigma_q^{-1}\}+ (\mu_p-\mu_q)^T\Sigma_q^{-1}(\mu_p-\mu_q)
\end{aligned}\]

<p>Combining all this we get,</p>

\[D_\text{KL}(p||q)=\frac{1}{2}\left[\log\frac{|\Sigma_q|}{|\Sigma_p|}-d+(\mu_p-\mu_q)^T\Sigma_q^{-1}(\mu_p-\mu_q) +tr\{\Sigma_p \Sigma_q^{-1}\} \right]\]

<p>Moreover, if \(\Sigma_p=\sigma^2_p I, \Sigma_q=\sigma^2_q I\) we have,</p>

\[D_\text{KL}(p||q)=\frac{1}{2}\left[d\log\frac{\sigma_q^2}{\sigma_p^2}-d+\frac{||\mu_p-\mu_q||_2^2}{\sigma^2_q} + \frac{\sigma_p^2}{\sigma_q^2}d \right]\]

<h3 id="special-cases-for-optimal-posterior-variance">Special Cases for Optimal Posterior Variance</h3>

<p>With \(\Sigma_\theta(x_t,t)=\sigma_t^2\) and the KL Divergence formular in <a href="#kl-divergence-between-two-gaussian-distributions">the previous section</a>, we have</p>

\[\begin{aligned}
L_{t-1} &amp;:=\mathbb{E}_q [ D_\text{KL}(q(x_{t-1}|X_t,X_0)||p_\theta(x_{t-1}|X_t)) \\
&amp;=\frac{1}{2}\left[d\log\frac{\sigma_t^2}{\tilde\beta_t}-d+\frac{\mathbb{E}_q||\tilde\mu_t-\mu_\theta||_2^2}{\sigma^2_t} + \frac{\tilde\beta_t}{\sigma_t^2}d \right]
\end{aligned}\]

<p>Case 1: \(X_0\) is deterministic</p>

<p>When \(X_0\) is deterministically taking some value \(x_0\), \(L_{t-1}\) is minimized when \(\mu_\theta=\tilde\mu_t\) and \(\sigma_t^2=\tilde\beta_t\)</p>

<p>Case 2: \(X_0\sim\mathcal{N}(0,1)\)</p>

<p>If \(X_0\sim\mathcal{N}(0,1)\), then \(X_t=\sqrt{\overline{\alpha}_t}X_0+\sqrt{1-\overline\alpha_t}\overline\epsilon_t\sim\mathcal{N}(0,1)\) and \(\text{Cov}(X_t,X_0)=\sqrt{\overline{\alpha}_t}I\). Therefore, \(q(x_0\vert x_t)=\mathcal{N}(x_0;\sqrt{\overline{\alpha}_t}x_t,(1-\overline{\alpha}_t)I)\). To minimize the expectation of the KL divergence we set</p>

\[\begin{aligned}
\mu_\theta&amp;=\mathbb{E}[\tilde\mu_t|X_t]\\
&amp;=\frac{\sqrt{\overline\alpha_{t-1}}\beta_t}{1-\overline\alpha_t}\mathbb{E}[X_0|X_t]+\frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t}X_t\\
&amp;=\frac{\sqrt{\overline\alpha_{t-1}}\beta_t}{1-\overline\alpha_t}\sqrt{\overline\alpha_t}X_t+\frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t}X_t
\end{aligned}\]

<p>and therefore,</p>

\[\mathbb{E}_q||\tilde\mu_t-\mu_\theta||_2^2=\left(\frac{\sqrt{\overline\alpha_{t-1}}\beta_t}{1-\overline\alpha_t}\right)^2(1-\overline\alpha_t)d\]

<p>Differentiating \(L_{t-1}\) in this case with respect to \(\sigma_t^2\), and set to zero,</p>

\[\frac{\partial L_{t-1}}{\partial\sigma_t^2} = \frac{d}{2}\left[\frac{1}{\sigma_t^2}- \frac{\left(\frac{\sqrt{\overline\alpha_{t-1}}\beta_t}{1-\overline\alpha_t}\right)^2(1-\overline\alpha_t)+\tilde\beta_t}{\sigma_t^4}  \right]=0\]

<p>We get</p>

\[\begin{aligned}
\sigma_t^2&amp;=\left(\frac{\sqrt{\overline\alpha_{t-1}}\beta_t}{1-\overline\alpha_t}\right)^2(1-\overline\alpha_t)+\tilde\beta_t\\
&amp;=\left[\frac{\overline\alpha_{t-1}(1-\overline\alpha_t)\beta_t}{(1-\overline\alpha_t)^2}+\frac{1-\overline\alpha_{t-1}}{1-\overline\alpha_{t}}\right]\beta_t \\
&amp;=\left[\frac{\overline\alpha_{t-1}(1-\alpha_t)}{1-\overline\alpha_t}+\frac{1-\overline\alpha_{t-1}}{1-\overline\alpha_{t}}\right]\beta_t \\
&amp;=\beta_t
\end{aligned}\]]]></content><author><name></name></author><summary type="html"><![CDATA[Denoising Diffusion Probabilistic Models]]></summary></entry><entry><title type="html">T5: Text-to-Text Transfer Transformer</title><link href="https://dnabanita7.github.io/blog/2019/T5/" rel="alternate" type="text/html" title="T5: Text-to-Text Transfer Transformer" /><published>2019-10-23T00:00:00+00:00</published><updated>2019-10-23T00:00:00+00:00</updated><id>https://dnabanita7.github.io/blog/2019/T5</id><content type="html" xml:base="https://dnabanita7.github.io/blog/2019/T5/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>
    <p><strong>Text-to-text</strong> provides a simple way to train a single model on a wide variety of text tasks. T2T is simple, yet obtained comparable performance to task-specific architectures and ultimately produced SOTA results when combined with scale.</p>
  </li>
  <li>
    <p><strong>Architectures:</strong> The original encoder-decoder form worked best in T2T.</p>
  </li>
  <li>
    <p><strong>Unsupervised objectives:</strong> The denoising objectives performed best in T2T.</p>
  </li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Motivation: There is a need for a more rigorous understanding of the contributions of different components in transfer learning for NLP (large-scale pre-training models), e.g., different models, pre-training objectives, datasets, and fine-tuning methods.</p>

<p>The basic idea: Introduce a unified framework (T5) that converts all text-based language problems into a text-to-text format. The text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task considered.</p>

<p>This work primarily comprises a survey, exploration, and empirical comparison of existing techniques, and explores the limits of current approaches by scaling up the insights (training models up to 11 B parameters on dataset up to 750GB)</p>

<h2 id="methods">Methods</h2>

<h3 id="model">Model</h3>

<p>T5 closely follows the original Transformer<d-cite key="Transformer"></d-cite>.</p>

<p>Main differences:</p>
<ul>
  <li>LayerNorm
    <ul>
      <li>LayerNorms are used at the start of each block and the end of the last block.</li>
      <li>Scale-only LayerNorm, i.e., no additive bias.</li>
    </ul>
  </li>
  <li>Positional embedding
    <ul>
      <li>Relative positional embedding.</li>
      <li>Simplified position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights.</li>
      <li>Share the position embedding parameters across all layers in the model, though within a given layer each attention head uses a different learned position embedding.</li>
      <li>Use 32 embeddings with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding. (<a href="https://github.com/huggingface/transformers/blob/v4.25.1/src/transformers/models/t5/modeling_t5.py#L374">Implementation</a>)</li>
    </ul>
  </li>
  <li>Input embedding matrix
    <ul>
      <li>The weights of the output dense layer (before the final softmax) are shared with the input embedding matrix.</li>
    </ul>
  </li>
</ul>

<p>T5 uses an encoder-decoder architecture as in the original Transformer<d-cite key="Transformer"></d-cite>. In comparison, GPT<d-cite key="GPT"></d-cite>, GPT-2<d-cite key="GPT-2"></d-cite>, BERT<d-cite key="BERT"></d-cite> use a single stack of Transformer layers.</p>

<h3 id="dataset">Dataset</h3>

<p>The Colossal Clean Crawled Corpus (C4), ~ 750 GB.</p>

<ol>
  <li>Start with Common Crawl</li>
  <li>Retain lines that ended in a terminal punctuation mark.</li>
  <li>Discarded any page with fewer than 5 sentences and only retained lines that contained at least 3 words.</li>
  <li>Remove any page that contained any word on the “List of Dirty, Naughty, Obscene or Otherwise Bad Words”.</li>
  <li>Remove any line with the word Javascript.</li>
  <li>Remove any page where the phrase “lorem ipsum” (placeholder) appeared.</li>
  <li>Removed any pages that contained a curly bracket to avoid pages with code.</li>
  <li>Discarded all but one of any three-sentence span occurring more than once in the data set.</li>
  <li>Filter out non-English pages</li>
</ol>

<h3 id="input-and-output-format">Input and Output Format</h3>

<p>Cast all of the tasks considered into a “text-to-text” format, i.e., a task where the model is fed some text for context or conditioning and is then asked to produce some output text.</p>

<p>The text-to-text framework provides a consistent training objective both for pre-training and fine-tuning.</p>

<p>T5 is trained with a maximum likelihood objective (using “teacher forcing”, i.e., using ground truth as input, instead of model output from a prior time step as an input) and a cross-entropy loss regardless of the task. To specify which task the model should perform, a task-specific (text) prefix is added to the original input sequence before feeding it to the model.</p>

<p>Compare to GPT-2<d-cite key="GPT-2"></d-cite>, which also uses prompts:</p>

<ul>
  <li>GPT-2 is autoregressive (processing the prefix left-to-right), while T5 explicitly processes an input with an encoder (bidirectional attention).</li>
  <li>GPT-2 focuses on zero-shot learning, while T5 focuses on transfer learning with fine-tuning.</li>
</ul>

<h2 id="experiments">Experiments</h2>

<h3 id="baseline">Baseline</h3>

<h4 id="baseline-model">Baseline Model</h4>

<p>A standard encoder-decoder Transformer<d-cite key="Transformer"></d-cite> is designed so that the encoder and decoder are each similar in size and configuration to a BERT-base model.</p>

<h4 id="vocabulary">Vocabulary</h4>

<p>Use SentencePiece to encode text as WordPiece tokens (use a vocabulary of 32,000 wordpieces)</p>

<p>Trained the SentencePiece model on a mixture of 10 parts of English C4 data with 1 part each of data classified as German, French or Romanian. This vocabulary was shared across both the input and output of the model. Note that the vocabulary makes it so that the model can only process a predetermined, fixed set of languages.</p>

<h4 id="unsupervised-objective">Unsupervised Objective</h4>

<p>Use the “denoising” objectives, i.e., masked language modeling. The model is trained to predict missing or otherwise corrupted tokens in the input.</p>

<p>Design an objective that randomly samples and then drops out 15% of tokens in the input sequence. All consecutive spans of dropped-out tokens are replaced by a single sentinel token. Each sentinel token is assigned a token ID that is unique to the sequence.</p>

<p>The target then corresponds to all of the dropped-out spans of tokens, delimited by the same sentinel tokens used in the input sequence plus a final sentinel token to mark the end of the target sequence. An example is as follows.</p>

<p><em>Original text</em></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Thank you for inviting me to your party last week
</code></pre></div></div>

<p><em>Inputs</em></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Thank you for &lt;X&gt; to your party &lt;Y&gt; week
</code></pre></div></div>

<p><em>Target</em></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;X&gt; for inviting &lt;Y&gt; last &lt;Z&gt;
</code></pre></div></div>

<h3 id="architectures">Architectures</h3>

<p>Review and compare the following architectural variants.</p>

<div class="l-body" style="text-align:center;">
  <img src="https://media.arxiv-vanity.com/render-output/5540256/x4.png" width="80%" style="margin-bottom: 12px; background-color: white;" />
  <p>Different schematics of the Transformer architecture variants.</p>
</div>

<div class="l-body" style="text-align:center;">
  <img src="https://media.arxiv-vanity.com/render-output/5540256/x3.png" width="80%" style="margin-bottom: 12px; background-color: white;" />
  <p>Different attention mask patterns.</p>
</div>

<h4 id="model-structures">Model Structures</h4>

<p>A major distinguishing factor for different architectures is the “mask” used by different attention mechanisms in the model.</p>

<table>
  <thead>
    <tr>
      <th>Architectures</th>
      <th>mask</th>
      <th># of layer stacks</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Encoder-Decoder (e.g. T5)</td>
      <td>Encoder: Fully-visible, Decoder: Causal</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Language model (e.g. GPT)</td>
      <td>Causal</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Prefix LM</td>
      <td>Causal with prefix</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>A fundamental and frequently cited drawback of using an LM in the text-to-text setting is that causal masking forces the model’s representation of the \(i\)-th entry of the input sequence to only depend on the entries up until \(i\). This issue can be avoided in a Transformer-based language model simply by changing the masking pattern (Prefix LM).</p>

<p>The main difference between a prefix LM and the BERT architecture is that the classifier is simply integrated into the output layer of the Transformer decoder in the prefix LM.</p>

<h4 id="objectives">Objectives</h4>

<p>Considered both the standard language modeling objective and the denoising objective discussed in <a href="#unsupervised-objective">the previous section</a>.</p>

<p>Language modeling objective:</p>

<p>For models that ingest a prefix before making predictions (the encoder-decoder model and prefix LM), we sample a span of text from our unlabeled data set and choose a random point to split it into prefix and target portions.</p>

<p>For the standard language model, we train the model to predict the entire span from beginning to end.</p>

<p>Denoising objective:</p>

<p>The unsupervised denoising objective is designed for text-to-text models; to adapt it for use with a language model the inputs and targets are concatenated.</p>

<h4 id="results">Results</h4>

<ul>
  <li>For all tasks, the encoder-decoder architecture with the denoising objective performed best.</li>
  <li>Though an encoder-decoder model uses twice as many parameters as “encoder-only” (e.g. BERT) or “decoder-only” (language model) architectures, it has a similar computational cost.</li>
  <li>Sharing the parameters in the encoder and decoder did not result in a substantial performance drop while halving the total parameter count.</li>
</ul>

<h3 id="unsupervised-objectives">Unsupervised Objectives</h3>

<p>Explore different unsupervised objectives. Overall, all of the objectives ingest a sequence of token IDs corresponding to a tokenized span of text from our unlabeled text data set. The token sequence is processed to produce a (corrupted) input sequence and a corresponding target. Then, the model is trained as usual with maximum likelihood to predict the target sequence.</p>

<h4 id="choices-of-objectives">Choices of Objectives</h4>

<table>
  <thead>
    <tr>
      <th>Objective</th>
      <th>Example input</th>
      <th>Example target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Prefix LM</td>
      <td>Thank you for inviting</td>
      <td>me to your party last week .</td>
    </tr>
    <tr>
      <td>BERT-style</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> me to your party <strong>apple</strong> week .</td>
      <td><em>(original text)</em></td>
    </tr>
    <tr>
      <td>Deshuffling</td>
      <td>party me for your to . last fun you inviting week Thank</td>
      <td><em>(original text)</em></td>
    </tr>
    <tr>
      <td>MASS-style</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> me to your party <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> week .</td>
      <td><em>(original text)</em></td>
    </tr>
    <tr>
      <td>I.i.d. noise, replace spans</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> me to your party <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> week .</td>
      <td><code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> for inviting <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> last <code class="language-plaintext highlighter-rouge">&lt;Z&gt;</code></td>
    </tr>
    <tr>
      <td>I.i.d. noise, drop tokens</td>
      <td>Thank you me to your party week .</td>
      <td>for inviting last</td>
    </tr>
    <tr>
      <td>Random spans</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> to <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> week .</td>
      <td><code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> for inviting me <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> your party last <code class="language-plaintext highlighter-rouge">&lt;Z&gt;</code></td>
    </tr>
  </tbody>
</table>

<h4 id="results-1">Results</h4>

<ul>
  <li><strong>Denoising objectives</strong> outperformed language modeling and deshuffling for pre-training.</li>
  <li>No remarkable difference across the many variants of the denoising objectives.</li>
  <li>Different objectives can lead to different sequence lengths and thus different training speeds.</li>
</ul>

<h3 id="pre-training-dataset">Pre-Training Dataset</h3>

<ul>
  <li>Performance degrades as the data set size shrinks.</li>
  <li>When comparing C4 to data sets that use additional filtering, the authors found that training on in-domain unlabeled data could boost performance in a few downstream tasks. However, constraining to a single domain typically results in a smaller data set.</li>
  <li>Performance can degrade when an unlabeled data set is small enough that it is repeated many times over the course of pre-training. This motivates the use of a large and diverse data set like C4 for generic language understanding tasks.</li>
</ul>

<h3 id="training-strategy">Training Strategy</h3>

<h4 id="fine-tuning-methods">Fine-Tuning Methods</h4>

<p>The standard method is to fine-tune <em>all</em> parameters in the model.</p>

<p>Two alternative methods:</p>

<ul>
  <li><em>Adapter layers:</em> additional dense-ReLU-dense blocks are added after each of the preexisting feed-forward networks in each block of the Transformer. When fine-tuning, only the adapter layer and layer normalization parameters are updated.</li>
  <li><em>gradual unfreezing:</em> more and more of the model’s parameters are fine-tuned over time.</li>
</ul>

<p>The standard method performs best.</p>

<h4 id="multi-task-learning">Multi-Task Learning</h4>

<p>Train the model on multiple tasks simultaneously (the unsupervised task and downstream supervised tasks). For the unified text-to-text framework, “multi-task learning” simply corresponds to mixing data sets
together.</p>

<p>In general, multi-task training underperforms pre-training followed by fine-tuning on most tasks.</p>

<h4 id="combining-multi-task-learning-with-fine-tuning">Combining Multi-Task Learning with Fine-Tuning</h4>

<p>The model is pre-trained on all tasks at once but is then fine-tuned on the individual
supervised tasks.</p>

<p>Fine-tuning after multi-task pre-training results in comparable performance to the baseline (unsupervised pre-training + supervised fine-tuning). This suggests that using fine-tuning after multi-task learning can help mitigate some of the trade-offs between different mixing rates.</p>

<h3 id="scaling">Scaling</h3>

<p>Compared various strategies for taking advantage of additional computing, including training the model on more data, training a larger model, and using an ensemble of models. Each approach conferred a significant boost in performance. Specifically,</p>

<ul>
  <li>Increasing the training time and/or model size consistently improves the baseline.</li>
  <li>In general, increasing the model size resulted in an additional bump in performance compared to solely increasing the training time or batch size.</li>
  <li>Increasing the training time and increasing the model size can be complementary means of improving performance.</li>
  <li>Training a smaller model on more data was often outperformed by training a larger model for fewer steps.</li>
  <li>An ensemble of models can provide substantially better results than a single model, which provides an orthogonal means of leveraging additional computation.</li>
  <li>Ensembling models that were fine-tuned from the same base pre-trained model performed worse than pre-training and fine-tuning all models completely separately, though fine-tune-only ensembling still substantially outperformed a single model.</li>
  <li>Different scaling methods have different trade-offs that are separate from their performance.</li>
</ul>

<h3 id="putting-it-all-together">Putting It All Together</h3>

<p>The final T5 model is as follows.</p>

<ul>
  <li><strong>Objective:</strong> the span-corruption objective, a variant of the denoising objective.</li>
  <li><strong>Longer training:</strong> pre-train for 1M steps on a batch size of 2048 sequences of length 512 corresponding to a total of about 1T pre-training tokens.</li>
  <li><strong>Model sizes:</strong> up to 11B.</li>
  <li><strong>Multi-task pre-training + fine-tuning</strong></li>
  <li><strong>Beam search:</strong> replace greedy decoding by a beam search with a beam width of 4 and a length penalty of \(\alpha=0.6\).</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer]]></summary></entry><entry><title type="html">GPT-2</title><link href="https://dnabanita7.github.io/blog/2019/GPT-2/" rel="alternate" type="text/html" title="GPT-2" /><published>2019-02-14T00:00:00+00:00</published><updated>2019-02-14T00:00:00+00:00</updated><id>https://dnabanita7.github.io/blog/2019/GPT-2</id><content type="html" xml:base="https://dnabanita7.github.io/blog/2019/GPT-2/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>GPT-2 begins to learn different tasks without any explicit supervision (<strong>zero-shot transfer</strong>) when trained on a new dataset of millions of webpages (WebText).</li>
  <li>GPT-2 is able to perform new tasks by conditioning on text that specifies the task and the input (<strong>prompt learning</strong>).</li>
  <li>The capacity of the LM is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Supervised learning systems are brittle and sensitive to changes in distribution and task (“narrow experts”). The prevalence of single-task training on single-domain datasets might be a major contributor to the lack of generalization observed in current systems.</p>

<p>Multitask learning is a promising framework for improving general performance. However, multi-task training in NLP is still nascent.</p>

<ul>
  <li>Each (dataset, objective) pair is considered as a sample.</li>
  <li>Current ML systems need hundreds to thousands of examples to induce functions that generalize well.</li>
  <li>Difficult to scale with current approaches.</li>
</ul>

<p>The trend of pre-trained language representation NLP:</p>

<ol>
  <li>single-layer pre-trained word embedding + task-specific architectures</li>
  <li>multiple layers of representations (e.g. RNN) + task-specific architectures</li>
  <li>pre-train RNNs or Transformers, and then directly fine-tune, without task-specific architectures (e.g. GPT<d-cite key="GPT"></d-cite>, BERT<d-cite key="BERT"></d-cite>).</li>
</ol>

<p>This work: LM can perform a wide range of downstream tasks in a <strong>zero-shot</strong> setting, without any parameter or architecture modification.</p>

<h2 id="methods">Methods</h2>

<h3 id="language-model">Language Model</h3>

<p>Training LMs in a probabilistic framework as estimating a conditional distribution of the output given the input and the task information (multi-task/meta-learning),</p>

\[p(\texttt{output}|\texttt{input}, \texttt{task})\]

<p>The language model is <em>auto-regressive</em>, i.e., it predicts the next word given the previous words.</p>

<h4 id="task-conditioning">Task Conditioning</h4>

<ul>
  <li>architectural level, e.g., task-specific encoders and decoders</li>
  <li>algorithmic level, e.g., the inner and outer loop optimization framework of MAML</li>
  <li><strong>natural language</strong> provides a flexible way to specify tasks (prompt)</li>
</ul>

<h4 id="speculation">Speculation</h4>

<p>LMs with sufficient capacity will begin to learn to infer and perform the tasks demonstrated in natural language sequences in order to better predict them, regardless of their method of procurement. If LMs are able to do this it will be, in effect, performing unsupervised multitask learning.</p>

<p>Test this by analyzing the performance of LMs in a zero-shot setting on a wide variety of tasks.</p>

<h3 id="training-dataset">Training Dataset</h3>

<p>Although web scrapes such as Common Crawl are many orders of magnitude larger than current language modeling datasets, they have significant data quality issues.</p>

<p>The authors created a new web scrape that emphasizes document quality.</p>

<ol>
  <li>Only scraped web pages that have been curated/filtered by humans: scraped all outbound links from Reddit which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting, educational, or just funny.</li>
  <li>Extract the text from HTML responses</li>
  <li>Ee-duplication and some heuristic-based cleaning</li>
  <li>Removed all Wikipedia documents from WebText since it is a common data source for other datasets and could complicate analysis due to overlapping training data with test evaluation tasks.</li>
</ol>

<p>Results in over 8 million documents for a total of 40 GB of text (about 40 Billion Bytes).</p>

<h3 id="input-representation">Input Representation</h3>

<p>A general LM should be able to compute the probability of (and also generate) any string.</p>

<p>While processing Unicode strings as a sequence of UTF-8 bytes elegantly fulfills this requirement, current byte-level LMs are not competitive with word-level LMs on large-scale datasets.</p>

<h4 id="byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</h4>

<p>An interpolation between word-level inputs for frequent symbol sequences and character-level inputs for infrequent symbol sequences.</p>

<p>BPE on Unicode code points: The size of the base vocabulary is too large (&gt; 130,000) compared to the 32,000 to 64,000 token vocabularies often used with BPE.</p>

<h4 id="bpe-on-byte-level">BPE on Byte Level</h4>

<ol>
  <li>A base vocabulary of size 256</li>
  <li>Naive BPE results in suboptimal merges due to the greedy strategy. To avoid this, the authors prevent BPE from merging across character categories, with an exception for spaces.</li>
  <li>Enable the model to assign a probability to any Unicode string.</li>
</ol>

<h3 id="model">Model</h3>

<p>The model largely follows the details of the GPT<d-cite key="GPT"></d-cite> model with a few modifications.</p>

<ol>
  <li>LayerNorm was moved to the input of each sub-block and an additional LayerNorm was added after the final self-attention block.</li>
  <li>A modified initialization that accounts for the accumulation on the residual path with model depth is used. Scale the weights of residual layers at initialization by a factor of \(1/\sqrt{N}\), where \(N\) is the number of residual layers.</li>
  <li>The vocabulary is expanded to 50,257.</li>
  <li>We also increase the context size from 512 to 1024 tokens,</li>
  <li>A larger batch size of 512 is used.</li>
</ol>

<p>The largest model (GPT-2) has 1.5B parameters.</p>

<h2 id="experiments">Experiments</h2>

<h3 id="language-modeling">Language Modeling</h3>

<p>This is the primary task the models are trained for.</p>

<p>Task: Evaluating the log probability of different datasets according to the LM.</p>

<p>Results: GPT-2 transfers well across domains and datasets, improving the state of the art on 7 out of the 8 datasets in a zero-shot setting.</p>

<h3 id="lambada">LAMBADA</h3>

<p>The LAMBADA dataset tests the ability of systems to model long-range dependencies in text. The</p>

<p>Task: predict the final word of sentences that require at least 50 tokens of context for a human to successfully predict.</p>

<p>Results: GPT-2 improves the SOTA.</p>

<p>Common error: valid continuations of the sentence, but not valid final words.</p>

<p>This suggests that the LM is not using the additional useful constraint that the word must be the final of the sentence. Adding a stop-word filter as an approximation to this further increases accuracy.</p>

<h3 id="reading-comprehension">Reading Comprehension</h3>

<p>The Conversation Question Answering dataset (CoQA) consists of documents from 7 different domains paired with natural language dialogues between a question asker and a question answerer about the document. CoQA tests reading comprehension capabilities and also the ability of models to answer questions that depend on conversation history (such as “Why?”).</p>

<p>Use greedy decoding from GPT-2 conditioned on a document, the history of the associated conversation, and a final token <code class="language-plaintext highlighter-rouge">A:</code>.</p>

<p>Results: GPT-2 matches or exceeds the performance of 3 out of 4 baseline systems, and underperforms the supervised SOTA (BERT-based)</p>

<h3 id="summarization">Summarization</h3>

<p>Induce summarization behavior by adding the text <code class="language-plaintext highlighter-rouge">TL;DR:</code> after the article and generating 100 tokens with Top-\(k\) random sampling with \(k = 2\) which reduces repetition and encourages more abstractive summaries than greedy decoding. The first 3 generated sentences in these 100 tokens are used as the summary.</p>

<p>Results:</p>

<ul>
  <li>GPT-2 often focuses on recent content from the article or confuses specific details.</li>
  <li>GPT-2 only begins to approach the performance of classic neural baselines and just barely outperforms selecting 3 random sentences from the article.</li>
</ul>

<h3 id="translation">Translation</h3>

<p>Help GPT-2 infer the translation task, by conditioning the LM on a context of example pairs of the format <code class="language-plaintext highlighter-rouge">english sentence = french sentence</code> followed by a final prompt of <code class="language-plaintext highlighter-rouge">english sentence =</code>. After the prompt, outputs are sampled with greedy decoding and the first generated sentence is used as the translation.</p>

<p>Results:</p>

<ul>
  <li>English-French: GPT-2 is slightly worse than a word-by-word substitution with a bilingual lexicon.</li>
  <li>French-English: GPT-2 is able to leverage its very strong English LM and outperforms several unsupervised baselines but is still much worse than the SOTA unsupervised approach.</li>
</ul>

<p>Note: Since non-English webpages were filtered from WebText, it only contains a very small (10MB) corpus in the Frech language.</p>

<h3 id="question-answering">Question Answering</h3>

<p>Similar to translation, the context of the language model is seeded with example question answer pairs which helps the model infer the short answer style of the dataset.</p>

<p>Results: The performance of GPT-2 is still much, much, worse than the existing open domain question answering systems.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Language Models are Unsupervised Multitask Learners]]></summary></entry><entry><title type="html">GPT: Generative Pre-Training</title><link href="https://dnabanita7.github.io/blog/2018/GPT/" rel="alternate" type="text/html" title="GPT: Generative Pre-Training" /><published>2018-06-11T00:00:00+00:00</published><updated>2018-06-11T00:00:00+00:00</updated><id>https://dnabanita7.github.io/blog/2018/GPT</id><content type="html" xml:base="https://dnabanita7.github.io/blog/2018/GPT/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>Large gains can be realized by <em>generative pre-training</em> + <em>discriminative fine-tuning</em> on each specific task.</li>
  <li>Task-aware input transformations during fine-tuning achieve effective transfer while requiring minimal changes to the model architecture.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Learning from unlabeled data</p>

<ul>
  <li>Large labeled datasets are unavailable in many domains that suffer from a dearth of annotated resources.</li>
  <li>Large unlabeled text corpora are abundant</li>
  <li>Linguistic information from unlabeled data provides a valuable alternative to gathering more annotation.</li>
  <li>Even in cases where considerable supervision is available, learning good representations in an unsupervised fashion can provide a significant performance boost.</li>
</ul>

<p>Challenges</p>

<ul>
  <li>What type of optimization objectives are most effective at learning
text representations that are useful for transfer?</li>
  <li>No consensus on the most effective way to transfer these learned representations to the target task</li>
</ul>

<p>This work: unsupervised pre-training + supervised fine-tuning</p>

<h2 id="methods">Methods</h2>

<div class="l-body" style="text-align:center;">
  <img src="https://d3i71xaburhd42.cloudfront.net/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035/4-Figure1-1.png" width="100%" style="margin-bottom: 12px; background-color: white;" />
  <p><b>Left:</b> Transformer architecture and training objectives. <b>Right:</b> input
transformations for fine-tuning on different tasks.</p>
</div>

<h3 id="unsupervised-pre-training">Unsupervised Pre-Training</h3>

<h4 id="objective">Objective</h4>

<p>Standard language modeling:</p>

\[L_1(\mathcal{U}) = \sum_i\log P(u_i|u_{i-k},\dots, u_{i-1}; \Theta)\]

<p>where \(\mathcal{U}=\{u_1,\dots,u_n\}\) is an unlabeled corpus of tokens, \(k\) is the size of the context window, and \(\Theta\) is the parameters of network.</p>

<h4 id="model">Model</h4>

<p>Multi-layer Transformer decoder.</p>

<h4 id="dataset">Dataset</h4>

<p>The BooksCorpus dataset is used for pre-training, which contains over 7,000 unique unpublished books from a variety of genres.</p>

<h3 id="supervised-fine-tuning">Supervised Fine-Tuning</h3>

<p>After pre-training the model, adapt the parameters to the supervised target task.</p>

<p>Let \(\mathcal{C}\) be a labeled dataset where each instance consists of a sequence of input tokens, \(x^1,\dots,x^m\), and a label \(y\). The inputs are passed through the pre-trained model to obtain the final transformer block’s activation \(h_l^m\),  which is then fed into an added linear output layer with parameters \(W_y\) to predict \(y\):</p>

\[P(y|x^1,\dots, x^m)=\texttt{softmax}(h_l^m W_y).\]

<p>Use log-loss:</p>

\[L_2(\mathcal{C}) = \sum_{(x,y)}\log P(y|x^1,\dots, x^m)\]

<p>Including language modeling as an auxiliary objective to the fine-tuning, in order to</p>
<ul>
  <li>improve generalization of the supervised model,</li>
  <li>accelerate convergence.</li>
</ul>

<p>Total loss:</p>

\[L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda L_1(\mathcal{C}).\]

<h3 id="task-specific-input-transformations">Task-Specific Input Transformations</h3>

<p>For some tasks, like text classification, the inputs can be used as is.</p>

<p>Since tje pre-trained model was trained on contiguous sequences of text, some modifications are required for tasks with different formats of inputs, e.g., sentence pairs, triplets of document, question, and answers.</p>

<p>All transformations include adding randomly initialized start and end tokens (<code class="language-plaintext highlighter-rouge">&lt;s&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;e&gt;</code>).</p>

<h4 id="textual-entailment">Textual Entailment</h4>

<p>concatenate the premise \(p\) and hypothesis \(h\) token sequences, with a delimiter token (<code class="language-plaintext highlighter-rouge">$</code>) in between.</p>

<h4 id="similarity">Similarity</h4>

<p>There is no inherent ordering of the two sentences being compared. Modify the input sequence to contain both possible sentence orderings (with a delimiter in between) and process each independently to produce two sequence representations which are added element-wise before being fed into the linear output layer.</p>

<h4 id="question-answering-and-commonsense-reasoning">Question Answering and Commonsense Reasoning</h4>

<p>Concatenate the document context and question with each possible answer, adding a delimiter token in between to get \([z; q; \$; a_k]\). Each of these sequences is processed independently with the model and then normalized via a softmax layer to produce an output distribution over possible answers.</p>

<h2 id="experiments">Experiments</h2>

<h3 id="results-of-fine-tuning">Results of Fine-Tuning</h3>

<p>Overall, GPT achieves new SOTA results in 9 out of the 12 datasets, outperforming ensembles in many cases. Results also indicate that GPT works well across datasets of different sizes.</p>

<h3 id="impact-of-number-of-layers-transferred">Impact of Number of Layers Transferred</h3>

<p>Transferring embeddings improves performance and each transformer layer provides further benefits. This indicates that each layer in the pre-trained model contains useful functionality for solving target tasks.</p>

<h3 id="zero-shot-behaviors">Zero-shot Behaviors</h3>

<h4 id="hypothesis">Hypothesis</h4>

<p>The underlying generative model learns to perform many of the tasks in order to improve its language modeling capability and the more structured attentional memory of the transformer assists in transfer compared to LSTMs.</p>

<h4 id="test">Test</h4>

<p>Evaluate the <em>zero-shot</em> performance over the course of pre-training.</p>

<p>The zero-shot performance is stable and steadily increases over training suggesting that generative pretraining supports the learning of a wide variety of task-relevant functionality. Also, the LSTM exhibits higher variance in its zero-shot performance suggesting that the inductive bias of the Transformer architecture assists in transfer.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Improving Language Understanding by Generative Pre-Training]]></summary></entry><entry><title type="html">Gumbel-Softmax</title><link href="https://dnabanita7.github.io/blog/2017/Gumbel-Softmax/" rel="alternate" type="text/html" title="Gumbel-Softmax" /><published>2017-08-05T00:00:00+00:00</published><updated>2017-08-05T00:00:00+00:00</updated><id>https://dnabanita7.github.io/blog/2017/Gumbel-Softmax</id><content type="html" xml:base="https://dnabanita7.github.io/blog/2017/Gumbel-Softmax/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>Categorical latent variables can not backpropagate through samples.</li>
  <li>Replace the non-differentiable sample from a categorical distribution with a differentiable sample from a <strong>Gumbel-Softmax</strong> distribution.</li>
  <li>The Gumbel-Softmax distribution has the essential property that it can be <strong>smoothly annealed into</strong> a categorical distribution.</li>
</ul>

<h2 id="the-gumbel-distribution">The Gumbel Distribution</h2>

<p>Notation: \(X\sim\text{Gumbel}(\mu, \beta)\), where \(\mu\in\mathbb{R}\) is the location parameter and \(\beta&gt;0\) is the scale parameter.</p>

<p>PDF:</p>

\[f_X(x)=\frac{1}{\beta}e^{-(z+e^{-z})}, \text{ where } z=\frac{x-\mu}{\beta}.\]

<p>CDF:</p>

\[F_X(x)=e^{-e^{-z}}, \text{ where } z=\frac{x-\mu}{\beta}.\]

<p>See <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Wiki</a> for more details.</p>

<h2 id="the-gumbel-max-trick">The Gumbel-Max Trick</h2>

<p>Let \(\pi=(\pi_1,\dots,\pi_k)\) be \(k\)-d nonnegative vector, where not all elements are zero, and let \(g_1,\dots,g_k\) be \(k\) iid samples from \(\text{Gumbel}(0,1)\). Then</p>

\[\arg\max_i(g_i+\log\pi_i)\sim\text{Categorical}\left(\frac{\pi_j}{\sum_i\pi_i}\right)_j\]

<p>Proof:</p>

<p>Let \(I = \arg\max_i\{G_i + \log\pi_i\}\) and \(M = \max_i\{G_i + \log\pi_i\}\).</p>

\[\begin{aligned}
\mathbb{P}(I=i)&amp;=\mathbb{P}(G_i + \log\pi_i &lt; M, \forall j\neq i) \\
&amp; = \int_{-\infty}^\infty f_{G_i}(m-\log\pi_i) \prod_{j\neq i} F_{G_j}(m-\log\pi_j) dm \\
&amp; = \int_{-\infty}^\infty \exp(\log\pi_i-m-\exp(\log\pi_i-m)) \prod_{j\neq i} \exp(-\exp(\log\pi_j-m)) dm \\
&amp; = \int_{-\infty}^\infty \exp(\log\pi_i-m)\exp(-\exp(\log\pi_i-m)) \prod_{j\neq i} \exp(-\exp(\log\pi_j-m)) dm \\
&amp; = \int_{-\infty}^\infty \exp(\log\pi_i-m) \prod_{j} \exp(-\exp(\log\pi_j-m)) dm \\
&amp; = \int_{-\infty}^\infty \exp(\log\pi_i-m) \exp(-\sum_{j}\exp(\log\pi_j-m)) dm \\
&amp; = \int_{-\infty}^\infty \exp(\log\pi_i)\exp(-m) \exp(-\exp(-m)\sum_{j}\exp(\log\pi_j)) dm \\
&amp; = \int_{-\infty}^\infty \pi_i\exp(-m) \exp(-\exp(-m)\sum_{j}\pi_j) dm \\
&amp; = \int_{0}^\infty \pi_i \exp(-x\sum_{j}\pi_j) dx \\
&amp; = \frac{\pi_i}{\sum_j\pi_j}
\end{aligned}\]

<h2 id="the-gumbel-softmax-distribution">The Gumbel-Softmax Distribution</h2>

<p>Relax the Gumbel-Max trick by replacing argmax with softmax (continuous, differentiable) and generate \(k\)-d sample vectors</p>

\[y_i = \frac{\exp((\log(\pi_i)+g_i)/\tau)}{\sum_{j=1}^k\exp((\log(\pi_j)+g_j)/\tau)}.\]

<p>PDF:</p>

\[f_{Y_1,\dots,Y_k}(y_1,\dots,y_k;\pi,\tau)=\Gamma(k)\tau^{k-1}\left( \sum_{i=1}^k \pi_i/y_i^\tau \right)^{-k}\prod_{i=1}^k(\pi_i/y_i^{\tau+1}).\]

<ul>
  <li>The Gumbel-Softmax distribution interpolates between discrete one-hot-encoded categorical distributions and continuous categorical densities.</li>
  <li>For low temperatures, the expected value of a Gumbel-Softmax random variable approaches the expected value of a categorical random variable with the same logits.</li>
  <li>As the temperature increases, the expected value converges to a uniform distribution over the categories.</li>
  <li>Samples from GumbelSoftmax distributions are identical to samples from a categorical distribution as \(\tau\rightarrow 0\).</li>
  <li>At higher temperatures, Gumbel-Softmax samples are no longer one-hot, and become uniform as \(\tau\rightarrow\infty\).</li>
</ul>

<h2 id="the-gumbel-softmax-estimator">The Gumbel-Softmax Estimator</h2>

<p>The Gumbel-Softmax distribution is smooth for \(\tau &gt; 0\), and therefore has a well-defined gradient \(\partial y/\partial \pi\) with respect to the parameters \(\pi\). Thus, by replacing categorical samples with Gumbel-Softmax samples we can use backpropagation to compute gradients.</p>

<p>Denote the procedure of replacing non-differentiable categorical samples with a differentiable approximation during training as the <strong>Gumbel-Softmax estimator</strong>.</p>

<p>A tradeoff between small and large temperatures:</p>

<ul>
  <li>Small \(\tau\): Close to one-hot but the variance of the gradients is large</li>
  <li>Large \(\tau\): Samples are smooth but the variance of the gradients is small.</li>
</ul>

<p>In practice</p>

<ul>
  <li>Start at a high temperature and anneal to a small but non-zero temperature.</li>
  <li>Or let \(\tau\) be a trainable parameter (can be interpreted as entropy regularization).</li>
</ul>

<h2 id="the-straight-through-gumbel-softmax-estimator">The Straight-Through Gumbel-Softmax Estimator</h2>

<p>For scenarios that are constrained to sampling discrete values</p>

<ul>
  <li>Discretize \(y\) using argmax.</li>
  <li>But use the continuous approximation in the backward pass.</li>
</ul>

<p>Call this Straight-Through (ST) Gumbel-Softmax Estimator.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Categorical Reparameterization with Gumbel-Softmax]]></summary></entry></feed>