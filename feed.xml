<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://dnabanita7.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dnabanita7.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-04-05T03:03:25+00:00</updated><id>https://dnabanita7.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Gumbel-Softmax</title><link href="https://dnabanita7.github.io/blog/2017/Gumbel-Softmax/" rel="alternate" type="text/html" title="Gumbel-Softmax" /><published>2017-08-05T00:00:00+00:00</published><updated>2017-08-05T00:00:00+00:00</updated><id>https://dnabanita7.github.io/blog/2017/Gumbel-Softmax</id><content type="html" xml:base="https://dnabanita7.github.io/blog/2017/Gumbel-Softmax/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>Categorical latent variables can not backpropagate through samples.</li>
  <li>Replace the non-differentiable sample from a categorical distribution with a differentiable sample from a <strong>Gumbel-Softmax</strong> distribution.</li>
  <li>The Gumbel-Softmax distribution has the essential property that it can be <strong>smoothly annealed into</strong> a categorical distribution.</li>
</ul>

<h2 id="the-gumbel-distribution">The Gumbel Distribution</h2>

<p>Notation: \(X\sim\text{Gumbel}(\mu, \beta)\), where \(\mu\in\mathbb{R}\) is the location parameter and \(\beta&gt;0\) is the scale parameter.</p>

<p>PDF:</p>

\[f_X(x)=\frac{1}{\beta}e^{-(z+e^{-z})}, \text{ where } z=\frac{x-\mu}{\beta}.\]

<p>CDF:</p>

\[F_X(x)=e^{-e^{-z}}, \text{ where } z=\frac{x-\mu}{\beta}.\]

<p>See <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Wiki</a> for more details.</p>

<h2 id="the-gumbel-max-trick">The Gumbel-Max Trick</h2>

<p>Let \(\pi=(\pi_1,\dots,\pi_k)\) be \(k\)-d nonnegative vector, where not all elements are zero, and let \(g_1,\dots,g_k\) be \(k\) iid samples from \(\text{Gumbel}(0,1)\). Then</p>

\[\arg\max_i(g_i+\log\pi_i)\sim\text{Categorical}\left(\frac{\pi_j}{\sum_i\pi_i}\right)_j\]

<p>Proof:</p>

<p>Let \(I = \arg\max_i\{G_i + \log\pi_i\}\) and \(M = \max_i\{G_i + \log\pi_i\}\).</p>

\[\begin{aligned}
\mathbb{P}(I=i)&amp;=\mathbb{P}(G_i + \log\pi_i &lt; M, \forall j\neq i) \\
&amp; = \int_{-\infty}^\infty f_{G_i}(m-\log\pi_i) \prod_{j\neq i} F_{G_j}(m-\log\pi_j) dm \\
&amp; = \int_{-\infty}^\infty \exp(\log\pi_i-m-\exp(\log\pi_i-m)) \prod_{j\neq i} \exp(-\exp(\log\pi_j-m)) dm \\
&amp; = \int_{-\infty}^\infty \exp(\log\pi_i-m)\exp(-\exp(\log\pi_i-m)) \prod_{j\neq i} \exp(-\exp(\log\pi_j-m)) dm \\
&amp; = \int_{-\infty}^\infty \exp(\log\pi_i-m) \prod_{j} \exp(-\exp(\log\pi_j-m)) dm \\
&amp; = \int_{-\infty}^\infty \exp(\log\pi_i-m) \exp(-\sum_{j}\exp(\log\pi_j-m)) dm \\
&amp; = \int_{-\infty}^\infty \exp(\log\pi_i)\exp(-m) \exp(-\exp(-m)\sum_{j}\exp(\log\pi_j)) dm \\
&amp; = \int_{-\infty}^\infty \pi_i\exp(-m) \exp(-\exp(-m)\sum_{j}\pi_j) dm \\
&amp; = \int_{0}^\infty \pi_i \exp(-x\sum_{j}\pi_j) dx \\
&amp; = \frac{\pi_i}{\sum_j\pi_j}
\end{aligned}\]

<h2 id="the-gumbel-softmax-distribution">The Gumbel-Softmax Distribution</h2>

<p>Relax the Gumbel-Max trick by replacing argmax with softmax (continuous, differentiable) and generate \(k\)-d sample vectors</p>

\[y_i = \frac{\exp((\log(\pi_i)+g_i)/\tau)}{\sum_{j=1}^k\exp((\log(\pi_j)+g_j)/\tau)}.\]

<p>PDF:</p>

\[f_{Y_1,\dots,Y_k}(y_1,\dots,y_k;\pi,\tau)=\Gamma(k)\tau^{k-1}\left( \sum_{i=1}^k \pi_i/y_i^\tau \right)^{-k}\prod_{i=1}^k(\pi_i/y_i^{\tau+1}).\]

<ul>
  <li>The Gumbel-Softmax distribution interpolates between discrete one-hot-encoded categorical distributions and continuous categorical densities.</li>
  <li>For low temperatures, the expected value of a Gumbel-Softmax random variable approaches the expected value of a categorical random variable with the same logits.</li>
  <li>As the temperature increases, the expected value converges to a uniform distribution over the categories.</li>
  <li>Samples from GumbelSoftmax distributions are identical to samples from a categorical distribution as \(\tau\rightarrow 0\).</li>
  <li>At higher temperatures, Gumbel-Softmax samples are no longer one-hot, and become uniform as \(\tau\rightarrow\infty\).</li>
</ul>

<h2 id="the-gumbel-softmax-estimator">The Gumbel-Softmax Estimator</h2>

<p>The Gumbel-Softmax distribution is smooth for \(\tau &gt; 0\), and therefore has a well-defined gradient \(\partial y/\partial \pi\) with respect to the parameters \(\pi\). Thus, by replacing categorical samples with Gumbel-Softmax samples we can use backpropagation to compute gradients.</p>

<p>Denote the procedure of replacing non-differentiable categorical samples with a differentiable approximation during training as the <strong>Gumbel-Softmax estimator</strong>.</p>

<p>A tradeoff between small and large temperatures:</p>

<ul>
  <li>Small \(\tau\): Close to one-hot but the variance of the gradients is large</li>
  <li>Large \(\tau\): Samples are smooth but the variance of the gradients is small.</li>
</ul>

<p>In practice</p>

<ul>
  <li>Start at a high temperature and anneal to a small but non-zero temperature.</li>
  <li>Or let \(\tau\) be a trainable parameter (can be interpreted as entropy regularization).</li>
</ul>

<h2 id="the-straight-through-gumbel-softmax-estimator">The Straight-Through Gumbel-Softmax Estimator</h2>

<p>For scenarios that are constrained to sampling discrete values</p>

<ul>
  <li>Discretize \(y\) using argmax.</li>
  <li>But use the continuous approximation in the backward pass.</li>
</ul>

<p>Call this Straight-Through (ST) Gumbel-Softmax Estimator.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Categorical Reparameterization with Gumbel-Softmax]]></summary></entry></feed>