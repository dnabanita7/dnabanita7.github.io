<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://dnabanita7.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dnabanita7.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-04-05T03:02:11+00:00</updated><id>https://dnabanita7.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">T5: Text-to-Text Transfer Transformer</title><link href="https://dnabanita7.github.io/blog/2019/T5/" rel="alternate" type="text/html" title="T5: Text-to-Text Transfer Transformer" /><published>2019-10-23T00:00:00+00:00</published><updated>2019-10-23T00:00:00+00:00</updated><id>https://dnabanita7.github.io/blog/2019/T5</id><content type="html" xml:base="https://dnabanita7.github.io/blog/2019/T5/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>
    <p><strong>Text-to-text</strong> provides a simple way to train a single model on a wide variety of text tasks. T2T is simple, yet obtained comparable performance to task-specific architectures and ultimately produced SOTA results when combined with scale.</p>
  </li>
  <li>
    <p><strong>Architectures:</strong> The original encoder-decoder form worked best in T2T.</p>
  </li>
  <li>
    <p><strong>Unsupervised objectives:</strong> The denoising objectives performed best in T2T.</p>
  </li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Motivation: There is a need for a more rigorous understanding of the contributions of different components in transfer learning for NLP (large-scale pre-training models), e.g., different models, pre-training objectives, datasets, and fine-tuning methods.</p>

<p>The basic idea: Introduce a unified framework (T5) that converts all text-based language problems into a text-to-text format. The text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task considered.</p>

<p>This work primarily comprises a survey, exploration, and empirical comparison of existing techniques, and explores the limits of current approaches by scaling up the insights (training models up to 11 B parameters on dataset up to 750GB)</p>

<h2 id="methods">Methods</h2>

<h3 id="model">Model</h3>

<p>T5 closely follows the original Transformer<d-cite key="Transformer"></d-cite>.</p>

<p>Main differences:</p>
<ul>
  <li>LayerNorm
    <ul>
      <li>LayerNorms are used at the start of each block and the end of the last block.</li>
      <li>Scale-only LayerNorm, i.e., no additive bias.</li>
    </ul>
  </li>
  <li>Positional embedding
    <ul>
      <li>Relative positional embedding.</li>
      <li>Simplified position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights.</li>
      <li>Share the position embedding parameters across all layers in the model, though within a given layer each attention head uses a different learned position embedding.</li>
      <li>Use 32 embeddings with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding. (<a href="https://github.com/huggingface/transformers/blob/v4.25.1/src/transformers/models/t5/modeling_t5.py#L374">Implementation</a>)</li>
    </ul>
  </li>
  <li>Input embedding matrix
    <ul>
      <li>The weights of the output dense layer (before the final softmax) are shared with the input embedding matrix.</li>
    </ul>
  </li>
</ul>

<p>T5 uses an encoder-decoder architecture as in the original Transformer<d-cite key="Transformer"></d-cite>. In comparison, GPT<d-cite key="GPT"></d-cite>, GPT-2<d-cite key="GPT-2"></d-cite>, BERT<d-cite key="BERT"></d-cite> use a single stack of Transformer layers.</p>

<h3 id="dataset">Dataset</h3>

<p>The Colossal Clean Crawled Corpus (C4), ~ 750 GB.</p>

<ol>
  <li>Start with Common Crawl</li>
  <li>Retain lines that ended in a terminal punctuation mark.</li>
  <li>Discarded any page with fewer than 5 sentences and only retained lines that contained at least 3 words.</li>
  <li>Remove any page that contained any word on the “List of Dirty, Naughty, Obscene or Otherwise Bad Words”.</li>
  <li>Remove any line with the word Javascript.</li>
  <li>Remove any page where the phrase “lorem ipsum” (placeholder) appeared.</li>
  <li>Removed any pages that contained a curly bracket to avoid pages with code.</li>
  <li>Discarded all but one of any three-sentence span occurring more than once in the data set.</li>
  <li>Filter out non-English pages</li>
</ol>

<h3 id="input-and-output-format">Input and Output Format</h3>

<p>Cast all of the tasks considered into a “text-to-text” format, i.e., a task where the model is fed some text for context or conditioning and is then asked to produce some output text.</p>

<p>The text-to-text framework provides a consistent training objective both for pre-training and fine-tuning.</p>

<p>T5 is trained with a maximum likelihood objective (using “teacher forcing”, i.e., using ground truth as input, instead of model output from a prior time step as an input) and a cross-entropy loss regardless of the task. To specify which task the model should perform, a task-specific (text) prefix is added to the original input sequence before feeding it to the model.</p>

<p>Compare to GPT-2<d-cite key="GPT-2"></d-cite>, which also uses prompts:</p>

<ul>
  <li>GPT-2 is autoregressive (processing the prefix left-to-right), while T5 explicitly processes an input with an encoder (bidirectional attention).</li>
  <li>GPT-2 focuses on zero-shot learning, while T5 focuses on transfer learning with fine-tuning.</li>
</ul>

<h2 id="experiments">Experiments</h2>

<h3 id="baseline">Baseline</h3>

<h4 id="baseline-model">Baseline Model</h4>

<p>A standard encoder-decoder Transformer<d-cite key="Transformer"></d-cite> is designed so that the encoder and decoder are each similar in size and configuration to a BERT-base model.</p>

<h4 id="vocabulary">Vocabulary</h4>

<p>Use SentencePiece to encode text as WordPiece tokens (use a vocabulary of 32,000 wordpieces)</p>

<p>Trained the SentencePiece model on a mixture of 10 parts of English C4 data with 1 part each of data classified as German, French or Romanian. This vocabulary was shared across both the input and output of the model. Note that the vocabulary makes it so that the model can only process a predetermined, fixed set of languages.</p>

<h4 id="unsupervised-objective">Unsupervised Objective</h4>

<p>Use the “denoising” objectives, i.e., masked language modeling. The model is trained to predict missing or otherwise corrupted tokens in the input.</p>

<p>Design an objective that randomly samples and then drops out 15% of tokens in the input sequence. All consecutive spans of dropped-out tokens are replaced by a single sentinel token. Each sentinel token is assigned a token ID that is unique to the sequence.</p>

<p>The target then corresponds to all of the dropped-out spans of tokens, delimited by the same sentinel tokens used in the input sequence plus a final sentinel token to mark the end of the target sequence. An example is as follows.</p>

<p><em>Original text</em></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Thank you for inviting me to your party last week
</code></pre></div></div>

<p><em>Inputs</em></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Thank you for &lt;X&gt; to your party &lt;Y&gt; week
</code></pre></div></div>

<p><em>Target</em></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;X&gt; for inviting &lt;Y&gt; last &lt;Z&gt;
</code></pre></div></div>

<h3 id="architectures">Architectures</h3>

<p>Review and compare the following architectural variants.</p>

<div class="l-body" style="text-align:center;">
  <img src="https://media.arxiv-vanity.com/render-output/5540256/x4.png" width="80%" style="margin-bottom: 12px; background-color: white;" />
  <p>Different schematics of the Transformer architecture variants.</p>
</div>

<div class="l-body" style="text-align:center;">
  <img src="https://media.arxiv-vanity.com/render-output/5540256/x3.png" width="80%" style="margin-bottom: 12px; background-color: white;" />
  <p>Different attention mask patterns.</p>
</div>

<h4 id="model-structures">Model Structures</h4>

<p>A major distinguishing factor for different architectures is the “mask” used by different attention mechanisms in the model.</p>

<table>
  <thead>
    <tr>
      <th>Architectures</th>
      <th>mask</th>
      <th># of layer stacks</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Encoder-Decoder (e.g. T5)</td>
      <td>Encoder: Fully-visible, Decoder: Causal</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Language model (e.g. GPT)</td>
      <td>Causal</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Prefix LM</td>
      <td>Causal with prefix</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>A fundamental and frequently cited drawback of using an LM in the text-to-text setting is that causal masking forces the model’s representation of the \(i\)-th entry of the input sequence to only depend on the entries up until \(i\). This issue can be avoided in a Transformer-based language model simply by changing the masking pattern (Prefix LM).</p>

<p>The main difference between a prefix LM and the BERT architecture is that the classifier is simply integrated into the output layer of the Transformer decoder in the prefix LM.</p>

<h4 id="objectives">Objectives</h4>

<p>Considered both the standard language modeling objective and the denoising objective discussed in <a href="#unsupervised-objective">the previous section</a>.</p>

<p>Language modeling objective:</p>

<p>For models that ingest a prefix before making predictions (the encoder-decoder model and prefix LM), we sample a span of text from our unlabeled data set and choose a random point to split it into prefix and target portions.</p>

<p>For the standard language model, we train the model to predict the entire span from beginning to end.</p>

<p>Denoising objective:</p>

<p>The unsupervised denoising objective is designed for text-to-text models; to adapt it for use with a language model the inputs and targets are concatenated.</p>

<h4 id="results">Results</h4>

<ul>
  <li>For all tasks, the encoder-decoder architecture with the denoising objective performed best.</li>
  <li>Though an encoder-decoder model uses twice as many parameters as “encoder-only” (e.g. BERT) or “decoder-only” (language model) architectures, it has a similar computational cost.</li>
  <li>Sharing the parameters in the encoder and decoder did not result in a substantial performance drop while halving the total parameter count.</li>
</ul>

<h3 id="unsupervised-objectives">Unsupervised Objectives</h3>

<p>Explore different unsupervised objectives. Overall, all of the objectives ingest a sequence of token IDs corresponding to a tokenized span of text from our unlabeled text data set. The token sequence is processed to produce a (corrupted) input sequence and a corresponding target. Then, the model is trained as usual with maximum likelihood to predict the target sequence.</p>

<h4 id="choices-of-objectives">Choices of Objectives</h4>

<table>
  <thead>
    <tr>
      <th>Objective</th>
      <th>Example input</th>
      <th>Example target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Prefix LM</td>
      <td>Thank you for inviting</td>
      <td>me to your party last week .</td>
    </tr>
    <tr>
      <td>BERT-style</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> me to your party <strong>apple</strong> week .</td>
      <td><em>(original text)</em></td>
    </tr>
    <tr>
      <td>Deshuffling</td>
      <td>party me for your to . last fun you inviting week Thank</td>
      <td><em>(original text)</em></td>
    </tr>
    <tr>
      <td>MASS-style</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> me to your party <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> week .</td>
      <td><em>(original text)</em></td>
    </tr>
    <tr>
      <td>I.i.d. noise, replace spans</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> me to your party <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> week .</td>
      <td><code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> for inviting <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> last <code class="language-plaintext highlighter-rouge">&lt;Z&gt;</code></td>
    </tr>
    <tr>
      <td>I.i.d. noise, drop tokens</td>
      <td>Thank you me to your party week .</td>
      <td>for inviting last</td>
    </tr>
    <tr>
      <td>Random spans</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> to <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> week .</td>
      <td><code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> for inviting me <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> your party last <code class="language-plaintext highlighter-rouge">&lt;Z&gt;</code></td>
    </tr>
  </tbody>
</table>

<h4 id="results-1">Results</h4>

<ul>
  <li><strong>Denoising objectives</strong> outperformed language modeling and deshuffling for pre-training.</li>
  <li>No remarkable difference across the many variants of the denoising objectives.</li>
  <li>Different objectives can lead to different sequence lengths and thus different training speeds.</li>
</ul>

<h3 id="pre-training-dataset">Pre-Training Dataset</h3>

<ul>
  <li>Performance degrades as the data set size shrinks.</li>
  <li>When comparing C4 to data sets that use additional filtering, the authors found that training on in-domain unlabeled data could boost performance in a few downstream tasks. However, constraining to a single domain typically results in a smaller data set.</li>
  <li>Performance can degrade when an unlabeled data set is small enough that it is repeated many times over the course of pre-training. This motivates the use of a large and diverse data set like C4 for generic language understanding tasks.</li>
</ul>

<h3 id="training-strategy">Training Strategy</h3>

<h4 id="fine-tuning-methods">Fine-Tuning Methods</h4>

<p>The standard method is to fine-tune <em>all</em> parameters in the model.</p>

<p>Two alternative methods:</p>

<ul>
  <li><em>Adapter layers:</em> additional dense-ReLU-dense blocks are added after each of the preexisting feed-forward networks in each block of the Transformer. When fine-tuning, only the adapter layer and layer normalization parameters are updated.</li>
  <li><em>gradual unfreezing:</em> more and more of the model’s parameters are fine-tuned over time.</li>
</ul>

<p>The standard method performs best.</p>

<h4 id="multi-task-learning">Multi-Task Learning</h4>

<p>Train the model on multiple tasks simultaneously (the unsupervised task and downstream supervised tasks). For the unified text-to-text framework, “multi-task learning” simply corresponds to mixing data sets
together.</p>

<p>In general, multi-task training underperforms pre-training followed by fine-tuning on most tasks.</p>

<h4 id="combining-multi-task-learning-with-fine-tuning">Combining Multi-Task Learning with Fine-Tuning</h4>

<p>The model is pre-trained on all tasks at once but is then fine-tuned on the individual
supervised tasks.</p>

<p>Fine-tuning after multi-task pre-training results in comparable performance to the baseline (unsupervised pre-training + supervised fine-tuning). This suggests that using fine-tuning after multi-task learning can help mitigate some of the trade-offs between different mixing rates.</p>

<h3 id="scaling">Scaling</h3>

<p>Compared various strategies for taking advantage of additional computing, including training the model on more data, training a larger model, and using an ensemble of models. Each approach conferred a significant boost in performance. Specifically,</p>

<ul>
  <li>Increasing the training time and/or model size consistently improves the baseline.</li>
  <li>In general, increasing the model size resulted in an additional bump in performance compared to solely increasing the training time or batch size.</li>
  <li>Increasing the training time and increasing the model size can be complementary means of improving performance.</li>
  <li>Training a smaller model on more data was often outperformed by training a larger model for fewer steps.</li>
  <li>An ensemble of models can provide substantially better results than a single model, which provides an orthogonal means of leveraging additional computation.</li>
  <li>Ensembling models that were fine-tuned from the same base pre-trained model performed worse than pre-training and fine-tuning all models completely separately, though fine-tune-only ensembling still substantially outperformed a single model.</li>
  <li>Different scaling methods have different trade-offs that are separate from their performance.</li>
</ul>

<h3 id="putting-it-all-together">Putting It All Together</h3>

<p>The final T5 model is as follows.</p>

<ul>
  <li><strong>Objective:</strong> the span-corruption objective, a variant of the denoising objective.</li>
  <li><strong>Longer training:</strong> pre-train for 1M steps on a batch size of 2048 sequences of length 512 corresponding to a total of about 1T pre-training tokens.</li>
  <li><strong>Model sizes:</strong> up to 11B.</li>
  <li><strong>Multi-task pre-training + fine-tuning</strong></li>
  <li><strong>Beam search:</strong> replace greedy decoding by a beam search with a beam width of 4 and a length penalty of \(\alpha=0.6\).</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer]]></summary></entry><entry><title type="html">Gumbel-Softmax</title><link href="https://dnabanita7.github.io/blog/2017/Gumbel-Softmax/" rel="alternate" type="text/html" title="Gumbel-Softmax" /><published>2017-08-05T00:00:00+00:00</published><updated>2017-08-05T00:00:00+00:00</updated><id>https://dnabanita7.github.io/blog/2017/Gumbel-Softmax</id><content type="html" xml:base="https://dnabanita7.github.io/blog/2017/Gumbel-Softmax/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>Categorical latent variables can not backpropagate through samples.</li>
  <li>Replace the non-differentiable sample from a categorical distribution with a differentiable sample from a <strong>Gumbel-Softmax</strong> distribution.</li>
  <li>The Gumbel-Softmax distribution has the essential property that it can be <strong>smoothly annealed into</strong> a categorical distribution.</li>
</ul>

<h2 id="the-gumbel-distribution">The Gumbel Distribution</h2>

<p>Notation: \(X\sim\text{Gumbel}(\mu, \beta)\), where \(\mu\in\mathbb{R}\) is the location parameter and \(\beta&gt;0\) is the scale parameter.</p>

<p>PDF:</p>

\[f_X(x)=\frac{1}{\beta}e^{-(z+e^{-z})}, \text{ where } z=\frac{x-\mu}{\beta}.\]

<p>CDF:</p>

\[F_X(x)=e^{-e^{-z}}, \text{ where } z=\frac{x-\mu}{\beta}.\]

<p>See <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Wiki</a> for more details.</p>

<h2 id="the-gumbel-max-trick">The Gumbel-Max Trick</h2>

<p>Let \(\pi=(\pi_1,\dots,\pi_k)\) be \(k\)-d nonnegative vector, where not all elements are zero, and let \(g_1,\dots,g_k\) be \(k\) iid samples from \(\text{Gumbel}(0,1)\). Then</p>

\[\arg\max_i(g_i+\log\pi_i)\sim\text{Categorical}\left(\frac{\pi_j}{\sum_i\pi_i}\right)_j\]

<p>Proof:</p>

<p>Let \(I = \arg\max_i\{G_i + \log\pi_i\}\) and \(M = \max_i\{G_i + \log\pi_i\}\).</p>

\[\begin{aligned}
\mathbb{P}(I=i)&amp;=\mathbb{P}(G_i + \log\pi_i &lt; M, \forall j\neq i) \\
&amp; = \int_{-\infty}^\infty f_{G_i}(m-\log\pi_i) \prod_{j\neq i} F_{G_j}(m-\log\pi_j) dm \\
&amp; = \int_{-\infty}^\infty \exp(\log\pi_i-m-\exp(\log\pi_i-m)) \prod_{j\neq i} \exp(-\exp(\log\pi_j-m)) dm \\
&amp; = \int_{-\infty}^\infty \exp(\log\pi_i-m)\exp(-\exp(\log\pi_i-m)) \prod_{j\neq i} \exp(-\exp(\log\pi_j-m)) dm \\
&amp; = \int_{-\infty}^\infty \exp(\log\pi_i-m) \prod_{j} \exp(-\exp(\log\pi_j-m)) dm \\
&amp; = \int_{-\infty}^\infty \exp(\log\pi_i-m) \exp(-\sum_{j}\exp(\log\pi_j-m)) dm \\
&amp; = \int_{-\infty}^\infty \exp(\log\pi_i)\exp(-m) \exp(-\exp(-m)\sum_{j}\exp(\log\pi_j)) dm \\
&amp; = \int_{-\infty}^\infty \pi_i\exp(-m) \exp(-\exp(-m)\sum_{j}\pi_j) dm \\
&amp; = \int_{0}^\infty \pi_i \exp(-x\sum_{j}\pi_j) dx \\
&amp; = \frac{\pi_i}{\sum_j\pi_j}
\end{aligned}\]

<h2 id="the-gumbel-softmax-distribution">The Gumbel-Softmax Distribution</h2>

<p>Relax the Gumbel-Max trick by replacing argmax with softmax (continuous, differentiable) and generate \(k\)-d sample vectors</p>

\[y_i = \frac{\exp((\log(\pi_i)+g_i)/\tau)}{\sum_{j=1}^k\exp((\log(\pi_j)+g_j)/\tau)}.\]

<p>PDF:</p>

\[f_{Y_1,\dots,Y_k}(y_1,\dots,y_k;\pi,\tau)=\Gamma(k)\tau^{k-1}\left( \sum_{i=1}^k \pi_i/y_i^\tau \right)^{-k}\prod_{i=1}^k(\pi_i/y_i^{\tau+1}).\]

<ul>
  <li>The Gumbel-Softmax distribution interpolates between discrete one-hot-encoded categorical distributions and continuous categorical densities.</li>
  <li>For low temperatures, the expected value of a Gumbel-Softmax random variable approaches the expected value of a categorical random variable with the same logits.</li>
  <li>As the temperature increases, the expected value converges to a uniform distribution over the categories.</li>
  <li>Samples from GumbelSoftmax distributions are identical to samples from a categorical distribution as \(\tau\rightarrow 0\).</li>
  <li>At higher temperatures, Gumbel-Softmax samples are no longer one-hot, and become uniform as \(\tau\rightarrow\infty\).</li>
</ul>

<h2 id="the-gumbel-softmax-estimator">The Gumbel-Softmax Estimator</h2>

<p>The Gumbel-Softmax distribution is smooth for \(\tau &gt; 0\), and therefore has a well-defined gradient \(\partial y/\partial \pi\) with respect to the parameters \(\pi\). Thus, by replacing categorical samples with Gumbel-Softmax samples we can use backpropagation to compute gradients.</p>

<p>Denote the procedure of replacing non-differentiable categorical samples with a differentiable approximation during training as the <strong>Gumbel-Softmax estimator</strong>.</p>

<p>A tradeoff between small and large temperatures:</p>

<ul>
  <li>Small \(\tau\): Close to one-hot but the variance of the gradients is large</li>
  <li>Large \(\tau\): Samples are smooth but the variance of the gradients is small.</li>
</ul>

<p>In practice</p>

<ul>
  <li>Start at a high temperature and anneal to a small but non-zero temperature.</li>
  <li>Or let \(\tau\) be a trainable parameter (can be interpreted as entropy regularization).</li>
</ul>

<h2 id="the-straight-through-gumbel-softmax-estimator">The Straight-Through Gumbel-Softmax Estimator</h2>

<p>For scenarios that are constrained to sampling discrete values</p>

<ul>
  <li>Discretize \(y\) using argmax.</li>
  <li>But use the continuous approximation in the backward pass.</li>
</ul>

<p>Call this Straight-Through (ST) Gumbel-Softmax Estimator.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Categorical Reparameterization with Gumbel-Softmax]]></summary></entry></feed>